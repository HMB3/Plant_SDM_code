#########################################################################################################################
#############################################  FIT MAXENT TO GBIF DATA ################################################## 
#########################################################################################################################


#########################################################################################################################
## This code takes a table of all species occurrences (rows) and environmental values (columns), and runs SDMs for a list
## of taxa. First, targetted background selection is used. Then, backwards selection is used on the occurrence and 
## background points generated by the targetted selection.


## Print the species run to the screen
message('Running maxent models for ', length(GBIF.spp), ' species in the set ', "'", save_run, "'")


##########################################################################################################################
## Load SDM table.
if(read_data == "TRUE") {
  
  ## This table will contains all the records for all HIA species (~3.8k).
  ## SDM_SPAT_OCC_BG_ALL_EVREGREEN_JULY_2018 is the latest version of all the species
  ## SDM.SPAT.OCC.BG = readRDS(paste0(DATA_path, 'SDM_SPAT_OCC_BG_ALL_EXTRA_JULY_2018.rds')) 
  SDM.SPAT.OCC.BG = readRDS(paste0(DATA_path, 'SDM_SPAT_OCC_BG_ALL_EVREGREEN_JULY_2018.rds'))
  
} else {
  
  message(' skip file reading, not many species analysed')   ##
  
}





#########################################################################################################################
## 1). RUN SDMs USING ALL A-PRIORI VARIABLES FOR ALL SPECIES
#########################################################################################################################


#########################################################################################################################
## Run Maxent using a targetted selection of background points. 
## within 200km of existing points
## Within the same Koppen zone as the existing points
#SDM.SPAT.OCC.BG = readRDS(paste0(DATA_path, 'SDM_SPAT_OCC_BG_ALL_EVREGREEN_JULY_2018.rds'))
SDM.SPAT.OCC.BG = readRDS(paste0(DATA_path, 'SDM_SPAT_OCC_BG_ALL_EXTRA_JULY_2018.rds')) 


## Check the table has all the species
dim(SDM.SPAT.OCC.BG)
length(intersect(unique(SDM.SPAT.OCC.BG$searchTaxon), GBIF.spp))  ## Should be same length as GBIF.spp
projection(template.raster.1km);projection(SDM.SPAT.OCC.BG);projection(Koppen_1975_1km)


#########################################################################################################################
## Loop over all the species
lapply(GBIF.spp, function(species){

   ## Skip the species if the directory already exists, before the loop
   outdir <- maxent_dir
   
   ## Could also check the folder size, so folders with no contents aren't skipped eg
   ## && sum(file.info(dir_name)$size) < 1000 (EG 1MB)
   dir_name = file.path(maxent_path, gsub(' ', '_', species))
   if(dir.exists(dir_name)) {
     message('Skipping ', species, ' - already run.')
     invisible(return(NULL))

   }

   ## Create the directory for the species in progress,
   ## so other parallel runs don't run the same species
   dir.create(dir_name)
   file.create(file.path(dir_name, "in_progress.txt"))
   #write.csv(data.frame(), file.path(dir_name, "in_progress.txt"))

   ## Print the taxa being processed to screen
   if(species %in% SDM.SPAT.OCC.BG$searchTaxon) {
     message('Doing ', species)

     ## If a species doesn't have any inventory data, we don't need this step
     
     ## Subset the records to only the taxa being processed
     ## Also subset to the source : ALA+ GBIF, or ALA + GBIF + INV
     ## This is what is causing the proportional sampling to skip.........................................
     occurrence <- subset(SDM.SPAT.OCC.BG, searchTaxon == species)
     occurrence <- occurrence[grep(paste(OCC_SOURCE, collapse = '|'), occurrence$SOURCE, ignore.case = TRUE),]
     message('Using occ records from ', unique(occurrence$SOURCE))

     ## Now get the background points. These can come from any species, other than the modelled species.
     ## However, they should be limited to the same SOURCE as the occ data
     background <- subset(SDM.SPAT.OCC.BG, searchTaxon != species)
     background <- background[grep(paste(unique(occurrence$SOURCE), collapse = '|'), background$SOURCE, ignore.case = TRUE),]
     message('Using bg records from ', unique(background$SOURCE))

     ## Finally fit the models using FIT_MAXENT_TARG_BG. Also use tryCatch to skip any exceptions
     tryCatch(
       fit_maxent_targ_bg_back_sel(occ                     = occurrence,    ## name from the .rmd CV doc
                                   bg                      = background,    ## name from the .rmd CV doc
                                   sdm.predictors          = bs.predictors,
                                   name                    = species,
                                   outdir                  = maxent_dir,
                                   bsdir                   = bs_dir,
                                   backwards_sel           = "TRUE",
                                   cor_thr                 = 0.8,      ## The maximum allowable pairwise correlation between predictor variables
                                   pct_thr                 = 5,        ## The minimum allowable percent variable contribution
                                   k_thr                   = 4,        ## The minimum number of variables to be kept in the model.

                                   template.raster         = template.raster.1km,
                                   min_n                   = 20,            ## This should be higher...
                                   max_bg_size             = 70000,         ## could be 50k or lower, it just depends on the biogeography
                                   Koppen                  = Koppen_1975_1km,
                                   background_buffer_width = 200000,
                                   shapefiles              = TRUE,
                                   features                = 'lpq',
                                   replicates              = 5,
                                   responsecurves          = TRUE,
                                   shp_path                = "./data/base/CONTEXTUAL/",
                                   aus_shp                 = "aus_states.rds"),

       ## If the species fails, write a fail message to file. Can this be the fail message itself?
       error = function(cond) {

         ## How to put the message into the file?
         file.create(file.path(dir_name, "sdm_failed.txt"))
         message(species, ' failed')
         cat(cond$message, file=file.path(dir_name, "sdm_failed.txt"))
         warning(species, ': ', cond$message)
       })

   } else {

     message(species, ' skipped - no data.')         ## This condition ignores species which have no data...
     file.create(file.path(dir_name, "completed.txt"))

   }

   ## now add a file to the dir to denote that it has completed
   file.create(file.path(dir_name, "completed.txt"))

 })


#########################################################################################################################
## Create a cluster to speed up the local running of SDMs - there are multiple ways to do this, and probably better ones!
## In this example, we need to export absolutely everything we need to the cluster. Sounds like there's a cleaner way to do it
## https://stackoverflow.com/questions/47424367/error-in-check-for-remote-errors-val-5-nodes-produced-an-error-object-not-fo


## Export cluster might work better - this takes a long time to export a cluster for 5 cores, given the size of each 
## Object. How can this be sped up, or maybe saved in the environment
# message('Running SDMs on the cluster ')
# nclust      <- 5
# cluster     <- makeCluster(nclust)
# clusterExport(cluster, c('template.raster.1km', 'SDM.SPAT.OCC.BG', 'Koppen_1975_1km', 'aus_states.rds', ## objects
#                          'fit_maxent_targ_bg_back_sel', 'local_simplify',                               ## functions
#                          'maxent_path', 'maxent_dir', 'bs_dir', 'OCC_SOURCE'))                          ## variables
# 
# clusterEvalQ(cluster, {
# 
#   ## These packages are required by SDM
#   require(ff)
#   require(rgeos)
#   require(sp)
#   require(raster)
#   require(rJava)
#   require(dismo)
#   require(things)
# 
# })





#########################################################################################################################
## 2). TABULATE MAXENT STATISTICS - ADD NICHE DATA LATER
#########################################################################################################################


## The code that adds niche info is now in './R/COLLATE_MAXENT_RESULTS.R' 
## The code below is just for running on Katana
## Print the species run to the screen
message('Creating summary stats for ', length(GBIF.spp), ' species in the set ', "'", save_run, "'")


#########################################################################################################################
## First, make a list of all the species with models, then restrict them to just the models on the GBIF.spp list 
map_spp_list  = gsub(" ", "_", GBIF.spp)
map_spp_patt  = paste0(map_spp_list, collapse = "|")
message ("map_spp_list head:")
message (paste (head(map_spp_list), collapse=","))


#########################################################################################################################
## Now stop R from creating listing all the maxent files that have completed - this takes a long time
#message ("DEBUGDEBUG - remember to disable next line")
#map_spp_list = head (map_spp_list)
message(results_dir)
maxent.tables = lapply (map_spp_list, FUN = function (x) {paste(results_dir , x, "full/maxent_fitted.rds", sep="/")})


## How many species have been modelled?
message(paste("maxent.tables has this many entries:", length(maxent.tables)))
message(paste(head (maxent.tables), collapse=","))
sdm.exists = lapply(maxent.tables, FUN = function (x) {file.exists (x)})
sdm.exists = unlist(sdm.exists)


## Only list the intersection between the modelled species and 
message(paste(head(sdm.exists), collapse=","))
maxent.tables = maxent.tables[sdm.exists]


message (paste ("maxent.tables has this many entries:", length(maxent.tables)))
maxent.tables = stringr::str_subset(maxent.tables, map_spp_patt)
message (paste ("maxent.tables has this many entries:", length(maxent.tables)))
message (paste (head(maxent.tables), collapse=","))


#########################################################################################################################
## Now create a table of the results 
## x = maxent.tables[1]
MAXENT.RESULTS <- maxent.tables %>%         
  
  ## Pipe the list into lapply
  lapply(function(x) {
    
    ## We don't need to skip species that haven't been modelled
    x = gsub(paste0(results_dir, "/"), "", x) 
    message (x)
    
    #############################################################
    ## load the backwards selected model
    if (grepl("BS", results_dir)) {
      m = readRDS(paste0(results_dir, '/',  x))
      
    } else {
      ## Get the background records from any source
      m = readRDS(paste0(results_dir, '/',  x))$me_full
      
    }
    
    ## Get the number of Variables
    number.var  = length(m@lambdas) - 4   ## (the last 4 slots of the lambdas file are not variables)
    mxt.records = nrow(m@presence)
    
    ## Get variable importance
    m.vars    = ENMeval::var.importance(m)
    var.pcont = m.vars[rev(order(m.vars[["percent.contribution"]])),][["variable"]][1]
    pcont     = m.vars[rev(order(m.vars[["percent.contribution"]])),][["percent.contribution"]][1]
    
    var.pimp  = m.vars[rev(order(m.vars[["permutation.importance"]])),][["variable"]][1]
    pimp      = m.vars[rev(order(m.vars[["permutation.importance"]])),][["permutation.importance"]][1]
    
    ## Get maxent results columns to be used for model checking
    ## Including the omission rate here
    Training_AUC             = m@results["Training.AUC",]
    Number_background_points = m@results["X.Background.points",]
    Logistic_threshold       = m@results["X10.percentile.training.presence.Logistic.threshold",]
    Omission_rate            = m@results["X10.percentile.training.presence.training.omission",]
    
    ## Now rename the maxent columns that we will use in the results table
    d = data.frame(searchTaxon              = x, 
                   Maxent_records           = mxt.records,
                   Number_var               = number.var, 
                   Var_pcont                = var.pcont,
                   Per_cont                 = pcont,
                   Var_pimp                 = var.pimp,
                   Perm_imp                 = pimp,
                   Training_AUC,
                   Number_background_points,  
                   Logistic_threshold,
                   Omission_rate)
    
    ## Remove path gunk, and species
    d$Species     = NULL
    d$searchTaxon = gsub("/full/maxent_fitted.rds", "", d$searchTaxon)
    return(d)
    
  }) %>%
  
  ## Finally, bind all the rows together
  bind_rows


#########################################################################################################################
## Now create a list of the '10th percentile training presence Logistic threshold'. This is used in step 8 to threshold
## the maps to just areas above the threshold.
message ("MAXENT.RESULTS columns") 
message (paste (colnames (MAXENT.RESULTS)))
message (paste (nrow (MAXENT.RESULTS)))
summary(MAXENT.RESULTS["Logistic_threshold"])   
percent.10.log = as.list(MAXENT.RESULTS["Logistic_threshold"])  
percent.10.log = percent.10.log$Logistic_threshold


#########################################################################################################################
## Create a list of the omission files - again, don't do this for all the files, just the intersection
omission.tables = lapply (map_spp_list, FUN = function (x) {paste(results_dir , x, "full/species_omission.csv", sep="/")})
message (head (omission.tables))


## Only process the existing files 
om.exists = lapply (omission.tables, FUN = function (x) {file.exists (x)})
om.exists = unlist(om.exists)


omission.tables = omission.tables[om.exists]
message(head(omission.tables))


## Get the maxium TSS value using the omission data : use _training_ ommission data only
Max_tss <- sapply(omission.tables, function(file) {
  
  ## For eachg species, read in the training data
  d <- read.csv(file)
  i <- which.min(d$Training.omission + d$Fractional.area)
  
  c(Max_tss = 1 - min(d$Training.omission + d$Fractional.area),
    thr     = d$Corresponding.logistic.value[i])
  
})



#########################################################################################################################
## Add a species variable to the TSS results, so we can subset to just the species analysed
Max_tss        = as.data.frame(Max_tss)
MAXENT.RESULTS = cbind(MAXENT.RESULTS, Max_tss)
summary(MAXENT.RESULTS$Max_tss)
summary(MAXENT.RESULTS$Omission_rate)


## This is a summary of maxent output for current conditions
## All species should have AUC > 0.7
dim(MAXENT.RESULTS)
head(MAXENT.RESULTS, 20)[1:5]


#########################################################################################################################
## Now check the match between the species list, and the results list. 
length(intersect(map_spp_list, MAXENT.RESULTS$searchTaxon)) 
MAXENT.RESULTS  =  MAXENT.RESULTS[MAXENT.RESULTS$searchTaxon %in% map_spp_list , ] 
map_spp         = unique(MAXENT.RESULTS$searchTaxon)
length(map_spp);setdiff(sort(map_spp_list), sort(map_spp))


#########################################################################################################################
## Then make a list of all the directories containing the individual GCM rasters. This is used for combining the rasters
SDM.RESULTS.DIR <- map_spp %>%
  
  ## Pipe the list into lapply
  lapply(function(species) {
    
    ## Create the character string...
    m <-   sprintf('%s/%s/full/', results_dir, species)                ## path.backwards.sel
    m
    
  }) %>%
  
  ## Bind the list together
  c()

length(SDM.RESULTS.DIR)
SDM.RESULTS.DIR = unlist(SDM.RESULTS.DIR)


# ## Change the species column
# MAXENT.RESULTS$searchTaxon = gsub("_", " ", MAXENT.RESULTS$searchTaxon)


#########################################################################################################################
## OUTSTANDING SDM TASKS:
#########################################################################################################################


## 1). Check the code can be run through katana


#########################################################################################################################
#####################################################  TBC ############################################################## 
#########################################################################################################################
