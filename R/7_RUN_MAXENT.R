#########################################################################################################################
#############################################  FIT MAXENT TO GBIF DATA ################################################## 
#########################################################################################################################


#########################################################################################################################
## This code takes a table of all species occurrences (rows) and environmental values (columns), and runs SDMs for a list
## of taxa. First, targetted background selection is used. Then, backwards selection is used on the occurrence and 
## background points generated by the targetted selection.


## Print the species run to the screen
message('Running maxent models for ', length(GBIF.spp), ' species in the set ', "'", save_run, "'")


# #########################################################################################################################
# ## Load SDM table.
# if(read_data == "TRUE") {
#   
#   ## This table will be all the records for all HIA species (~3.8k). These data will be re-processed, 1000 species at
#   ## a time. Then they will be combined, and modelled 100 at a time. EG GBIF.spp[100].
#   ## So "save_run" for Katana needs to be a table containing all the species
#   ## SDM.SPAT.OCC.BG <- readRDS("H:/green_cities_sdm/data/ANALYSIS/SDM_SPAT_OCC_BG_HOLLOW_SPP.rds")
#   SDM.SPAT.OCC.BG = readRDS(paste0(DATA_path, 'SDM_SPAT_OCC_BG_',  save_run, '.rds'))
#   
# } else {
#   
#   message(' skip file reading, not many species analysed')   ##
#   SDM.SPAT.OCC.BG <- readRDS("H:/green_cities_sdm/data/ANALYSIS/SDM_SPAT_OCC_BG_HOLLOW_SPP.rds")
#   
# }





#########################################################################################################################
## 1). RUN SDMs USING ALL A-PRIORI VARIABLES FOR ALL SPECIES
#########################################################################################################################


#########################################################################################################################
## Run Maxent using a targetted selection of background points. 
## within 200km of existing points
## Within the same Koppen zone as the existing points
## Try to sample bg points from the three sources (ALA/GBIF/INV in the same proportions as the occurrece data).
dim(SDM.SPAT.OCC.BG)
length(intersect(unique(SDM.SPAT.OCC.BG$searchTaxon), GBIF.spp))  ## Should be same length as GBIF.spp
projection(template.raster.1km);projection(SDM.SPAT.OCC.BG);projection(Koppen_1975_1km)


## Can error messages be saved inside the text file......................................................................


#########################################################################################################################
## Loop over all the species spp = GBIF.spp[1]
lapply(GBIF.spp, function(spp){ 
  
  ## Skip the species if the directory already exists, before the loop
  outdir <- maxent_dir
  
  dir_name = file.path(maxent_path, gsub(' ', '_', spp))
  if(dir.exists(dir_name)) {
    message('Skipping ', spp, ' - already run.')
    invisible(return(NULL))
    
  }
  
  ## Create the directory for the species in progress, 
  ## so other parallel runs don't run the same species
  dir.create(dir_name)
  write.csv(data.frame(), file.path(dir_name, "in_progress.txt"))
  
  ## Print the taxa being processed to screen
  if(spp %in% SDM.SPAT.OCC.BG$searchTaxon) {
    message('Doing ', spp) 
    
    ## Subset the records to only the taxa being processed
    occurrence <- subset(SDM.SPAT.OCC.BG, searchTaxon == spp)
    
    ## Now get the background points. These can come from any spp, other than the modelled species.
    background <- subset(SDM.SPAT.OCC.BG, searchTaxon != spp)
    
    ## Finally fit the models using FIT_MAXENT_TARG_BG. Also use tryCatch to skip any exceptions
    tryCatch(
      fit_maxent_targ_bg_back_sel(occ                     = occurrence,    ## name from the .rmd CV doc 
                                  bg                      = background,    ## name from the .rmd CV doc  
                                  sdm.predictors          = bs.predictors, 
                                  name                    = spp, 
                                  outdir                  = maxent_dir,
                                  bsdir                   = bs_dir,
                                  
                                  template.raster         = template.raster.1km,
                                  min_n                   = 20,            ## This should be higher...
                                  max_bg_size             = 70000,         ## could be 50k or lower, it just depends on the biogeography
                                  Koppen                  = Koppen_1975_1km,
                                  background_buffer_width = 200000,
                                  shapefiles              = TRUE,
                                  features                = 'lpq',
                                  replicates              = 5,
                                  responsecurves          = TRUE,
                                  shp_path                = "./data/base/CONTEXTUAL/", 
                                  aus_shp                 = "aus_states.rds"),
      
      ## If the species fails, write a fail message to file. Can this be the fail message itself?
      error = function(cond) {
        
        message(spp, ' failed')  
        write.csv(data.frame(), file.path(dir_name, "failed.txt"))
        
      })
    
  } else {
    
    message(spp, ' skipped - no data.')         ## This condition ignores species which have no data...
    write.csv(data.frame(), file.path(dir_name, "completed.txt"))
    
  }  
  
  ## now add a file to the dir to denote that it has completed
  write.csv(data.frame(), file.path(dir_name, "completed.txt"))
  
})





#########################################################################################################################
## 2). TABULATE MAXENT STATISTICS - ADD NICHE DATA LATER
#########################################################################################################################


## The code that adds niche info is now in './R/COLLATE_MAXENT_RESULTS.R' 
## The code below is just for running on Katana
## Print the species run to the screen
message('Creating summary stats for ', length(GBIF.spp), ' species in the set ', "'", save_run, "'")


#########################################################################################################################
## First, create a file list for each model run: Try crunching this into just the species required
map_spp_list  = gsub(" ", "_", GBIF.spp)
maxent.tables = list.files(results_dir)                 
maxent.tables = intersect(maxent.tables, map_spp_list)   
results_dir   = results_dir                             
length(maxent.tables) 


#########################################################################################################################
## This section illustrates how to index the maxent model object, which could be applied to other methods     #---------#
## Create a table of the results 
## x = maxent.tables[1]
MAXENT.RESULTS <- maxent.tables %>%         
  
  ## Pipe the list into lapply
  lapply(function(x) {
    
    ## Create the character string
    f <- paste0(results_dir, "/",  x, "/full/maxentResults.csv")
    
    ## Load each .RData file
    d <- read.csv(f)
    
    #############################################################
    ## load model
    if (grepl("BS", results_dir)) {
      m = readRDS(sprintf('%s/%s/full/maxent_fitted.rds', results_dir, x))
      
    } else {
      ## Get the background records from any source
      m = readRDS(sprintf('%s/%s/full/maxent_fitted.rds', results_dir, x))$me_full
      
    }
    
    ## Get the number of Variables
    number.var  = length(m@lambdas) - 4   ## (the last 4 slots of the lambdas file are not variables)
    mxt.records = nrow(m@presence)
    
    ## Get variable importance
    m.vars    = ENMeval::var.importance(m)
    var.pcont = m.vars[rev(order(m.vars[["percent.contribution"]])),][["variable"]][1]
    pcont     = m.vars[rev(order(m.vars[["percent.contribution"]])),][["percent.contribution"]][1]
    
    var.pimp  = m.vars[rev(order(m.vars[["permutation.importance"]])),][["variable"]][1]
    pimp      = m.vars[rev(order(m.vars[["permutation.importance"]])),][["permutation.importance"]][1]
    
    ## Get maxent results columns to be used for model checking
    Training_AUC             = m@results["Training.AUC",]
    Number_background_points = m@results["X.Background.points",]
    Logistic_threshold       = m@results["X10.percentile.training.presence.Logistic.threshold",] 
    
    ## Now rename the maxent columns that we will use in the results table
    d = cbind(searchTaxon              = x, 
              Maxent_records           = mxt.records,
              Number_var               = number.var, 
              Var_pcont                = var.pcont,
              Per_cont                 = pcont,
              Var_pimp                 = var.pimp,
              Perm_imp                 = pimp,
              Training_AUC,
              Number_background_points,  
              Logistic_threshold,
              d)  
    dim(d)
    
    ## Remove path gunk, and species
    d$Species     = NULL
    return(d)
    
  }) %>%
  
  ## Finally, bind all the rows together
  bind_rows


#########################################################################################################################
## Now create a list of the '10th percentile training presence Logistic threshold'. This is used in step 8 to threshold
## the maps to just areas above the threshold.
summary(MAXENT.RESULTS["Logistic_threshold"])   
percent.10.log = as.list(MAXENT.RESULTS["Logistic_threshold"])  
percent.10.log = percent.10.log$Logistic_threshold


#########################################################################################################################
## Get the maxium TSS value using the omission data : use _training_ ommission data only
omission.tables = list.files(results_dir, pattern = 'species_omission\\.csv$', full.names = TRUE, recursive = TRUE)

max_tss <- sapply(omission.tables, function(file) {
  
  ## For eachg species, read in the training data
  d <- read.csv(file)
  i <- which.min(d$Training.omission + d$Fractional.area)
  
  c(max_tss = 1 - min(d$Training.omission + d$Fractional.area),
    thr     = d$Corresponding.logistic.value[i])
  
})


#########################################################################################################################
## Then extract the omission rate for each logistic threshold 
omission_rate <- mapply(function(file, threshold) {
  
  ## Read in species data
  d <- read.csv(file)
  d[findInterval(threshold, d$Training.omission),]$Training.omission
  
}, omission.tables, percent.10.log)


#########################################################################################################################
## Add a species variable to the TSS results, so we can subset to just the species analysed
max_tss  = as.data.frame(max_tss)
setDT(max_tss, keep.rownames = TRUE)[]
max_tss  = as.data.frame(max_tss)
names(max_tss)[names(max_tss) == 'rn'] <- 'searchTaxon'


omission_rate  = as.data.frame(omission_rate)
setDT(omission_rate, keep.rownames = TRUE)[]
omission_rate  = as.data.frame(omission_rate)
names(omission_rate)[names(omission_rate) == 'rn'] <- 'searchTaxon'


## Remove extra text
max_tss$searchTaxon = gsub("//",         "", max_tss$searchTaxon)
max_tss$searchTaxon = gsub(results_dir,   "", max_tss$searchTaxon)
max_tss$searchTaxon = gsub("/full/species_omission.csv.max_tss", "", max_tss$searchTaxon)
max_tss$searchTaxon = gsub("/",          "", max_tss$searchTaxon)
head(max_tss)

omission_rate$searchTaxon = gsub("//",         "", omission_rate$searchTaxon)
omission_rate$searchTaxon = gsub(results_dir,   "", omission_rate$searchTaxon)
omission_rate$searchTaxon = gsub("/full/species_omission.csv", "", omission_rate$searchTaxon)
omission_rate$searchTaxon = gsub("/",          "", omission_rate$searchTaxon)
head(omission_rate)


## Add max TSS to the results table
MAXENT.RESULTS = join(MAXENT.RESULTS, max_tss)
MAXENT.RESULTS = join(MAXENT.RESULTS, omission_rate)
summary(MAXENT.RESULTS$max_tss)
summary(MAXENT.RESULTS$omission_rate)


## This is a summary of maxent output for current conditions
## Also which species have AUC < 0.7?
dim(MAXENT.RESULTS)
head(MAXENT.RESULTS, 20)[1:5]
dim(subset(MAXENT.RESULTS, Training.AUC > 0.7))  ## all models should be above 0.7





#########################################################################################################################
## Plot AUC vs. TSS
if (nrow(MAXENT.RESULTS) > 2) {
  
  lm.auc = lm(MAXENT.RESULTS$max_tss ~ MAXENT.RESULTS$Training.AUC)
  
  ## Save this to file
  png(paste0('./output/maxent/', 'Maxent_run_summary_', save_run, '.png'), 16, 12, units = 'in', res = 500)
  
  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
  
  plot(MAXENT.RESULTS$Training.AUC, MAXENT.RESULTS$max_tss, pch = 19, col  = "blue",
       xlab = "AUC", ylab = "TSS", 
       abline(lm(MAXENT.RESULTS$max_tss ~ MAXENT.RESULTS$Training.AUC)), 
       main = save_run, cex = 3)
  
  legend("topleft", bty = "n", 
         legend = paste("R2 is", format(summary(lm.auc)$adj.r.squared, digits = 4)))
  
  hist(MAXENT.RESULTS$Training.AUC, breaks = 10, col = "blue",   border = FALSE,
       ylab = "Frequency",
       xlab = "Training AUC", main = "AUC", cex = 3)
  hist(MAXENT.RESULTS$max_tss,      breaks = 10, col = "orange", border = FALSE,
       ylab = "",
       xlab = "Maximum True Skill Statistic", main = "TSS", cex = 3)
  
  ## Finsish the device
  dev.off()
  
  ## If the species list is < 2 records, don't plot
} else {
  
  message('Dont plot, only ', length(GBIF.spp), ' species analysed')
  
}


#########################################################################################################################
## Now check the match between the species list, and the results list. 
length(intersect(map_spp_list, MAXENT.RESULTS$searchTaxon)) 
MAXENT.RESULTS  =  MAXENT.RESULTS[MAXENT.RESULTS$searchTaxon %in% map_spp_list , ] 
map_spp         = unique(MAXENT.RESULTS$searchTaxon)
length(map_spp);setdiff(sort(map_spp_list), sort(map_spp))


#########################################################################################################################
## Then make a list of all the directories containing the individual GCM rasters. This is used for combining the rasters
SDM.RESULTS.DIR <- map_spp %>%
  
  ## Pipe the list into lapply
  lapply(function(species) {
    
    ## Create the character string...
    m <-   sprintf('%s/%s/full/', results_dir, species)                ## path.backwards.sel
    m 
    
  }) %>%
  
  ## Bind the list together
  c()

length(SDM.RESULTS.DIR)
SDM.RESULTS.DIR = unlist(SDM.RESULTS.DIR)


#########################################################################################################################
## Which columns will help with model selection
MAXENT.RESULTS.SAVE = select(MAXENT.RESULTS, searchTaxon,   Maxent_records, Number_var, Var_pcont,
                             Per_cont,       Var_pimp,      Perm_imp,       Iterations, Training_AUC, 
                             max_tss,        omission_rate, Number_background_points,    Logistic_threshold)


## Rename the species so the niche info can be joined on later
MAXENT.RESULTS.SAVE$searchTaxon = gsub("_", " ", MAXENT.RESULTS.SAVE$searchTaxon)





#########################################################################################################################
## Save maxent results 
if(save_data == "TRUE") {
  
  ## save .rds file for the next session
  saveRDS(MAXENT.RESULTS.SAVE, paste0(DATA_path, 'MAXENT_RESULTS_', save_run, '.rds'))
  
} else {
  
  message('Dont save niche summary, only ', length(GBIF.spp), ' species analysed')
  
}


#########################################################################################################################
## OUTSTANDING SDM TASKS:
#########################################################################################################################


## 1). Check the code can be run through katana


#########################################################################################################################
#####################################################  TBC ############################################################## 
#########################################################################################################################