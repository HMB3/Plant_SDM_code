---
title: "Green Cities climate suitability project"
authors: "Hugh Burley, John Baumgartner, Linda Beaumont"
date: "December 2018"
output:
html_document: default
pdf_document: default
---

\

This R markdown file summarises my contribution to the 'Which Plant Where' project at Macquarie University, 
outlining the key functions I have created during my postdoctural work. WPW is part of the Green Cities
co-investment fund. 

https://horticulture.com.au/co-investment-fund/green-cities-fund/


https://www.whichplantwhere.com.au/ 


The text and code below summarises a workflow in R that can be used to relatively rapidly assess the effect
of climate change on a given taxon within Australia, from dowloading occurrence records, through to creating
maps of predicted climatic suitablity across Australia at 1km*1km resolution. An example for 10 species is
given in the markdown file.

\


#########################################################################################################
# **<span style="color:blue"> Summary </span>**
#########################################################################################################

\

**Aim:** Globally, local government authorities are increasing their investment in urban greening 
interventions, yet there is little consideration of whether the current palette of species for these 
plantings will be resilient to climate change. We assessed the distribution of climatically suitable 
habitat, now and in the future, for 176 of the tree species most commonly grown by nurseries and 
planted across Australia's urban landscapes. 

**Location:** Australian Significant Urban Areas 

**Time period:** 1960 - 2070

**Major taxa analysed:** 176 commonly planted and sold horticultural tree species

**Methods:** Species' occurrence records were obtained from myriad tree inventories and 
natural history collections, along with baseline climate data (WorldClim) and six scenarios for 
three time periods: 2030, 2050, 2070. We calibrated climatic suitability models (CSMs) for each 
species and projected these onto current and future climate scenarios. For each species, we 
calculated i) changes to the size of climatically suitable habitat across Australia, 
ii) future loss of currently suitable urban habitat and iii) future gain of habitat across all
urban areas that are currently unsuitable.

**Results:** For more than 50% of the tree species analysed, suitable habitat is projected to decline 
by 2070 to less than half of its current extent, with suitable habitat in urban regions projected 
to decline for ~90% of species.

**Main conclusions:** The number of species with suitable climate within the largest Australian 
urban areas is projected to progressively decline as climate change intensifies. Our results 
highlight changing patterns of climatic space for different species, indicating that a pro-active 
approach, utilizing multiple lines of evidence, is needed to identify new planting opportunities. 
We also identify key issues associated with the use of CSMs for species in urban environments.

\


#########################################################################################################
# **<span style="color:blue"> Methods </span>**
#########################################################################################################

#########################################################################################################
### **Defining significant Urban Areas** 
Our spatial units for estimating species' future climatic suitability are the 101 Australian Significant 
Urban Areas (SUAs). SUAs are defined by the Australian Bureau of Statistics (ABS) as containing at least 
10,000 people within a single labour market. The SUA boundaries were taken from the 2016 SUA shapefile 
from the ABS, and cover the full span of current Australian climate (Fig 1). 


\newpage 
![](output/figures/CV_figs/SUA_TEMP_ESA_PLOT.png)


\newpage 
![](output/figures/CV_figs/figMAT_PET_SUA.png)

**Figure 1.** Largest Significant Urban Areas in each Australian State and Territory 
(top panel). The distribution of SUAs in climate space [bottom panel, largest SUAs labelled. 
MAT = current mean annual temperature (worldclim 1960-1990), MAP = current annual 
precipitation, PET = current potential evaporation]. 

\

#########################################################################################################
## **Species selection**
To analyse horticulturally significant Australian species, we obtained a list of native tree species 
grown by the nursery industry across Australia, as well as the number of nurseries growing each species 
within each state or territory. We then created a spatial database of urban tree inventories in Australia 
by contacting local government authorities. This resulted in inventory data for 41 local government areas 
spanning  20 SUAs. The 400 most frequently reported trees within the inventories were then intersected 
with the nursery list (i.e. those trees currently being sold within any Australian nursery), creating 
a list of 248 native species. We consider these 248 species as the most commonly planted and sold native 
trees in the current Australian horticultural market. This species list was first checked against the 
backbone taxonomy of the Global Biodiversity Information Facilty (GBIF, www.gbif.org), and then against 
]The Plant List (TPL) backbone taxonomy using the Taxonstand package 
[version 2.1, Cayuela et al (2012)] in the R language [Version 3.5.1, R Core team (2018)]. The accepted 
names from the TPL taxonomy were then used for this analysis. 

\

```{r message=FALSE, echo = FALSE, warning = FALSE}

########################################################################################################
## Load data and packages
load("H:/green_cities_sdm/TEST_RUN.RData")
source("./R/READ_PACKAGES.R")


## List of Variables used to run the SDM analyses
GBIF.spp      = native.good.models[1:10]                  ## A list of species
GBIF.spp.rev  = sort(GBIF.spp, decreasing = TRUE)         ## the list reversed - used for local analyses

save_run      = "SUA_RMD"                                 ## A variable to append the run name to output files
map_spp_list  = gsub(" ", "_", GBIF.spp)                  ## Species list with "_" for mapping
map_spp_rev   = sort(map_spp_list, decreasing = TRUE)     ## Reversed, so we can run two at once

GBIF_path     = "./data/base/HIA_LIST/GBIF/OCC_SEARCH/"   ## The path where GBIF data is stored
ALA_path      = "./data/base/HIA_LIST/ALA/TREES_TEST/"    ## The path where ALA data is stored  place

maxent_path   = './output/maxent/SUA_TREES_ANALYSIS/'     ## The directory where files are saved               
maxent_dir    = 'output/maxent/SUA_TREES_ANALYSIS'        ## Another version of the path for the maxent functions
save_data     = 'TRUE'                                    ## Argument for saving the intermediary output - e.g. data frames
read_data     = 'FALSE'                                   ## Argument for saving the intermediary output - e.g. data frames
save_path     = 'data/base/HIA_LIST/COMBO'

```

\

#########################################################################################################
## **Occurrence data** 
For each of these planted and sold species, we downloaded global occurrence records from GBIF and the 
Atlas of Living Australia (ALA, www.ala.org.au) using the rgbif and ALA4R packages in R 
[https://github.com/AtlasOfLivingAustralia/ALA4R, (Hijmans et al. 2016) (R Core Team, 2017)]. 

\

```{r eval = FALSE, echo = TRUE}

## Download ALA data
download_ALA_all_species = function (species_list, path) {
  
  ## create variables
  skip.spp.list       = list()

  ## for every species in the list
  for(sp.n in species_list){
    
    ## 1). First, check if the f*&%$*# file exists
    file = paste0(path, sp.n, "_ALA_records.RData")
    
    ## If it's already downloaded, skip
    if (file.exists (file)) {
      
      print (paste ("file exists for species", sp.n, "skipping"))
      next
      
    }
    
    ## 2). Then check the spelling...incorrect nomenclature will return NULL result
    if (is.null(occurrences(taxon = sp.n, download_reason_id = 7)$data) == TRUE) {
      
      ## Now append the species which had incorrect nomenclature to the skipped list
      ## this is slow, but it works for now
      print (paste ("Possible incorrect nomenclature", sp.n, "skipping"))
      nomenclature = paste ("Possible incorrect nomenclature |", sp.n)
      skip.spp.list <- c(skip.spp.list, nomenclature)
      next
      
    }
    
    ## 3). Skip species with no records
    if (dim(occurrences(taxon = sp.n, download_reason_id = 7)$data)[1] <= 2) {
      
      ## now append the species which had no records to the skipped list
      print (paste ("No ALA records for", sp.n, "skipping"))
      records = paste ("No ALA records |", sp.n)
      skip.spp.list <- c(skip.spp.list, records)
      next
      
    }
    
    ## 4). Download ALL records from ALA :: 
    message("Downloading ALA records for ", sp.n, " using ALA4R :: occurrences")
    ALA = occurrences(taxon = sp.n, download_reason_id = 7) 
    ALA = ALA[["data"]]
    cat("Synonyms returned for :: ", sp.n, unique(ALA$scientificName), sep="\n")
    message(dim(ALA[1]), " Records returned for ", sp.n)
    
    ## 5). save records to .Rdata file
    save(ALA, file = paste(path, sp.n, "_ALA_records.RData", sep = ""))
    
  }
  
}

```

\

We then undertook an intensive spatial data-cleaning process to remove spatially invalid or suspect records 
that, if retained, can cause species' climate tolerances to be miscalculated. Spatially invalid records, 
those taken before 1950, duplicate records, those within 10 km of capital cities, herbaria and biodiversity 
institutions, and individual records > 300 km from other records were all removed using the CoordinateCleaner 
package (version 1.0-7). We then calibrated climate suitability models (CSMs) for all 248 native planted 
and sold tree species using best practices, as summarised below.

\

```{r echo = FALSE, message = FALSE, warning = FALSE}
## Combine GBIF occurrence data with ALA data and filter to records > 1950
source('./R/ALA_DATA_FILTER_TAXO_SCIENTIFIC_NAME.R', echo = FALSE)
source('./R/3)_GBIF_DATA_TAXO_SCIENTIFIC_NAME.R',    echo = FALSE)


## Combine GBIF, ALA and urban occurrence data into a single table, extract environmental condtions
source('./R/4)_ALA_GBIF_URBAN_COMBINE.R', echo = FALSE)
source('./R/INVENTORY_RASTER.R',          echo = FALSE)


## Clean the occurrence data using the 'CleanCoordinates' function in the CoordinateCleaner package to remove
## records near herbaria, duplicates, etc. & add contextual info for each record (taxonomic and horticultural)
## Then prepare the SDM table
## Then clean the spatial outliers
source('./R/5)_GBIF_ALA_CLEAN_NICHES.R',  echo = FALSE)
source('./R/6)_PREPARE_SDM_TABLE_1KM.R',  echo = FALSE)

```

\

#########################################################################################################
## **Climate data**
We obtained baseline climate data from the WorldClim Database [worldclim.org/bioclim, Version 1.4 
(Hijmans et al. 2005)]. WorldClim comprises 19 bioclimatic variables, summarised for the period 1960-1990, 
of which we used eight for model calibration: Annual mean temperature, Temperature seasonality, 
Maximum temperature of the warmest month, Minimum temperature of the coldest month, Annual precipitation, 
Precipitation seasonality, Precipitation of the wettest month, and Precipitation of the driest month. 
These variables were chosen to capture climate averages, seasonality and extremes, all of which have 
been identified as important variables for predicting suitable habitat for plants (Bradie & Leung, 2017). 

\

When assessing the impacts of climate change, it is more robust to use projections from multiple climate 
models (IPCC, 2014; Taylor, Stouffer, & Meehl, 2011; Beaumont et al. 2007, Baumgartner et al 2018). We 
utilised a subset of six models recommended by CSIRO's Climate Change in Australia report 
(https://www.climatechangeinaustralia.gov.au/en/), as these models were shown to perform better than others 
in their ability to simulate historical Australian climate (Table 1). Hence, more confidence can be placed 
in their projections of future climate. For each of these six GCMs, we downloaded monthly maximum and 
minimum temperature and monthly precipitation from CSIRO for 2030, 2050 and 2070, at a spatial resolution 
of 30 arc-seconds (~1 km). From these data, we calculated the eight bioclimatic variables for the three 
time periods and re-projected the data to an equal area grid of 1 km x 1 km resolution to match the baseline 
climate, using R [version 3.4.2, R Core Team (2017)]. 

\

#########################################################################################################
## **Modeling approach**
We modelled climatic suitability under current conditions (1960-1990) using the Maxent algorithm 
(Elith et al. 2011; Phillips et al. 2006; Phillips and Dudik 2008) within the Dismo R package 
(Hijmans et al. 2016). Maxent is a machine learning, correlative approach to modelling climatic suitability 
that is generally regarded as superior to other algorithms that are used to model presence-only 
occurrence data (Elith et al. 2006). To run models for all species, we created a function in the 
R programming language which splits a table of species records * environment into only those spatial 
records for a given species, and calibrates a Maxent model under current climate for that species.

\


```{r run maxent, eval = FALSE, echo = TRUE}

#########################################################################################################
## A function which splits a table of species records * enviromental conditions into records for each 
## species, then fits a maxent model under current climatic conditions.
lapply(GBIF.spp, function(spp){ 
  
  ## Skip the species if the directory already exists, before the loop
  outdir <- maxent_dir
  
  if(dir.exists(file.path(maxent_path, gsub(' ', '_', spp)))) {
    message('Skipping ', spp, ' - already run.')
    invisible(return(NULL))
    
  }
  
  ## Print the taxa being processed to screen
  if(spp %in% SDM.SPAT.ALL$searchTaxon) {
    message('Doing ', spp) 
    
    ## Subset the records to only the taxa being processed
    occurrence <- subset(SDM.SPAT.ALL, searchTaxon == spp)
    
    ## Now get the background points. These can come from any spp, other than the modelled species.
    background <- subset(SDM.SPAT.ALL, searchTaxon != spp)
    
    ## Finally fit the models using FIT_MAXENT_TARG_BG. 
    tryCatch(
      FIT_MAXENT_TARG_BG(occ                     = occurrence, 
                         bg                      = background, 
                         sdm.predictors          = sdm.select, 
                         name                    = spp, 
                         outdir, 
                         template.raster,
                         min_n                   = 20,            
                         max_bg_size             = 70000,         ## could be 50k or lower
                         Koppen                  = Koppen_1975,
                         background_buffer_width = 200000,
                         shapefiles              = TRUE,
                         features                = 'lpq',
                         replicates              = 5,
                         responsecurves          = TRUE),
      
      error = function(cond) {

        message(paste('Species skipped ', spp))

      })
    
  } else {
    
    message(spp, ' skipped - no data.')   
    
  }  
  
})


```

\


```{r eval = FALSE, echo = TRUE}

## Print the maxent function
FIT_MAXENT_TARG_BG <- function(occ,
                               bg, # A Spatial points data frame (SPDF) of candidate background points
                               sdm.predictors, # a vector of enviro conditions that you want to include
                               name,
                               outdir,
                               template.raster,
                               # template.raster is an empty raster with extent, 
                               # res and projection of final output rasters. 
                               # It is used to reduce occurrences to a single 
                               # point per cell.
                               min_n,
                               # min_n is the minimum number of records (unique cells)
                               # required for a model to be fit
                               max_bg_size,
                               background_buffer_width, # How many km to buffer
                               shapefiles,
                               features,
                               replicates, # number of cross-validation replicates
                               responsecurves,
                               rep_args,
                               full_args) {
  
  ########################################################################
  ## First, stop if the outdir file exists,
  if(!file.exists(outdir)) stop('outdir does not exist :(', call. = FALSE)
  outdir_sp <- file.path(outdir, gsub(' ', '_', name))
  
  if(!missing('Koppen')) {
    if(!is(Koppen, 'RasterLayer'))
      stop('Koppen must be a RasterLayer, should be the same coord system as template.raster')  
  }
  
  ## If the file doesn't exist, split out the features
  if(!file.exists(outdir_sp)) dir.create(outdir_sp)
  features <- unlist(strsplit(features, ''))
  
  ## Make sure user features are allowed: don't run the model if the
  ## features have been incorrectly specified in the main argument
  ## l: linear
  ## p: product
  ## q: quadratic
  ## h: hinge
  ## t: threshold
  if(length(setdiff(features, c('l', 'p', 'q', 'h', 't'))) > 1)
    stop("features must be a vector of one or more of ',
         'l', 'p', 'q', 'h', and 't'.")
  
  ## Aggregate
  b <- aggregate(gBuffer(occ, width = background_buffer_width, byid = TRUE))
  
  #####################################################################
  ## Get unique cell numbers for species occurrences
  cells <- cellFromXY(template.raster, occ)
  
  ## Clean out duplicate cells and NAs 
  ## Note this will get rid of a lot of duplicate records 
  not_dupes <- which(!duplicated(cells) & !is.na(cells))
  occ       <- occ[not_dupes, ]
  cells     <- cells[not_dupes]
  message(nrow(occ), ' occurrence records (unique cells).')
  
  #####################################################################
  ## Skip species that have less than a minimum number of records: eg 20 species
  if(nrow(occ) < min_n) {
    
    print (paste ('Fewer occurrence records than the number of cross-validation ',
                  'replicates for species ', name,
                  ' Model not fit for this species'))
    
  } else {
    
    ## Subset the background records to the 200km buffered polygon
    message(name, ' creating background cells')
    system.time(o <- over(bg, b))
    bg <- bg[which(!is.na(o)), ]
    bg_cells <- cellFromXY(template.raster, bg)
    
    ## Clean out duplicates and NAs (including points outside extent of predictor data)
    bg_not_dupes <- which(!duplicated(bg_cells) & !is.na(bg_cells))
    bg <- bg[bg_not_dupes, ]
    bg_cells <- bg_cells[bg_not_dupes]
    
    ## Find which of these cells fall within the Koppen-Geiger zones that the species occupies
    if(!missing('Koppen')) {
      
      ## Crop the Kopppen raster to the extent of the occurrences, and snap it
      message(name, ' intersecting background cells with Koppen zones')
      Koppen_crop <- crop(Koppen, occ, snap = 'out')
      
      ## Only extract and match those cells that overlap between koppen_cropp, occ and bg 
      zones               <- raster::extract(Koppen_crop, occ)
      cells_in_zones_crop <- Which(Koppen_crop %in% zones, cells = TRUE)
      cells_in_zones      <- cellFromXY(Koppen, xyFromCell(Koppen_crop, cells_in_zones_crop))
      bg_cells            <- intersect(bg_cells, cells_in_zones)
      i                   <- cellFromXY(template.raster, bg)
      bg                  <- bg[which(i %in% bg_cells), ]
      
    }
    
    ## Reduce background sample if it's larger than max_bg_size
    if (nrow(bg) > max_bg_size) {
      
      message(nrow(bg), ' target species background records, reduced to random ',
              max_bg_size, '.')
      
      bg <- bg[sample(nrow(bg), max_bg_size), ]  ## Change this to use 
      
    } else {
      
      message(nrow(bg), ' target species background records.')
      
    }
    
    #####################################################################
    ## Save occ and bg shapefiles objects for future reference
    save_name = gsub(' ', '_', name)
    if(shapefiles) {
      
      suppressWarnings({
        
        message(name, ' writing occ and bg shapefiles')
        writeOGR(SpatialPolygonsDataFrame(b, data.frame(ID = seq_len(length(b)))),
                 outdir_sp, paste0(save_name, '_bg_buffer'), 'ESRI Shapefile', overwrite_layer = TRUE)
        writeOGR(bg,  outdir_sp, paste0(save_name, '_bg'),   'ESRI Shapefile', overwrite_layer = TRUE)
        writeOGR(occ, outdir_sp, paste0(save_name, '_occ'),  'ESRI Shapefile', overwrite_layer = TRUE)
        
      })
      
    }
    
    ## Save the background and occurrence points as objects
    saveRDS(bg,  file.path(outdir_sp, paste0(save_name, '_bg.rds')))
    saveRDS(occ, file.path(outdir_sp, paste0(save_name, '_occ.rds')))
    
    #####################################################################
    swd_occ <- occ[, sdm.predictors]
    saveRDS(swd_occ, file.path(outdir_sp, paste0(save_name,'_occ_swd.rds')))
    
    swd_bg <- bg[, sdm.predictors]
    saveRDS(swd_bg, file.path(outdir_sp, paste0(save_name, '_bg_swd.rds')))
    
    ## Save shapefiles of the occurrence and background points
    if(shapefiles) {
      
      writeOGR(swd_occ, outdir_sp,  paste0(save_name, '_occ_swd'), 
               'ESRI Shapefile', overwrite_layer = TRUE)
      
      writeOGR(swd_bg,  outdir_sp,  paste0(save_name, '_bg_swd'),  
               'ESRI Shapefile', overwrite_layer = TRUE)
      
    }
    
    #####################################################################
    ## Combine occurrence and background data
    swd <- as.data.frame(rbind(swd_occ@data, swd_bg@data))
    saveRDS(swd, file.path(outdir_sp, 'swd.rds'))
    pa <- rep(1:0, c(nrow(swd_occ), nrow(swd_bg)))
    
    ## Now check the features arguments are correct
    off <- setdiff(c('l', 'p', 'q', 't', 'h'), features)
    
    ## 
    if(length(off) > 0) {
      
      off <- c(l = 'linear=false',    p = 'product=false', q = 'quadratic=false',
               t = 'threshold=false', h = 'hinge=false')[off]
      
    }
    
    off <- unname(off)
    
    if(replicates > 1) {
      
      if(missing(rep_args)) rep_args <- NULL
      
      ## Run MAXENT for x cross validation data splits of swd : so 5 replicaes, 0-4
      message(name, ' running xval maxent')
      me_xval <- maxent(swd, pa, path = file.path(outdir_sp, 'xval'),
                        args = c(paste0('replicates=', replicates),
                                 'responsecurves=true',
                                 'outputformat=logistic',
                                 off, paste(names(rep_args), rep_args, sep = '=')))
      
    }
    
    ## Runs the full maxent model - using all the data in swd
    ## This uses DISMO to output standard files, but the names can't be altered
    if(missing(full_args)) full_args <- NULL
    message(name, ' running full maxent')
    me_full <- maxent(swd, pa, path = file.path(outdir_sp, 'full'),
                      args = c(off, paste(names(full_args), full_args, sep = '='),
                               'responsecurves=true',
                               'outputformat=logistic'))
    
    ## Save the full model
    saveRDS(list(me_xval = me_xval, me_full = me_full, swd = swd, pa = pa, 
                 koppen_gridcode = as.character
                 (Koppen_zones$Koppen[match(unique(zones), Koppen_zones$GRIDCODE)])), 
            file.path(outdir_sp, 'full', 'maxent_fitted.rds'))
    
    #####################################################################
    ## Save the chart corrleation file too for the variable set
    png(sprintf('%s/%s/full/%s_%s.png', outdir,
                save_name, save_name, "predictor_correlation"),
        3236, 2000, units = 'px', res = 300)
    
    ## set margins
    par(mar   = c(3, 3, 5, 3),  ## b, l, t, r
        oma   = c(1.5, 1.5, 1.5, 1.5))
    
    ## Add detail to the response plot
    chart.Correlation(swd_occ@data,
                      histogram = TRUE, pch = 19) 

    ## Finish the device
    dev.off()
    
  }
  
}

```

\

The performance of each model was estimated by calculating the average test Area Under the Receiver 
Operating Characteristic curve (AUC, Swets, 1988) and the True Skill Statistic (TSS) through five-fold 
cross-validation. This involved splitting the occurrence data for each species into five subsets of 
roughly equal size (i.e. folds), fitting the model to four of the five folds and predicting to the 
fifth. This process was repeated until each fold was used four times for model fitting and once for 
model evaluation (Stone, 1974). Below is a plot and table of the results for the full models (i.e. 
using all the data). 

\

```{r maxent table, message = FALSE, echo = FALSE, warning = FALSE}
source('./R/MAXENT_TABLE.R', echo = FALSE)
kable(MAXENT.RESULTS[c("searchTaxon", "X.Background.points", "Var_pimp", "Perm_imp", "Training.AUC",  "max_tss")])

```

\

#########################################################################################################
## **Combining the  models**
We then created a function to take the maxent model for each species, and project it onto the six climate 
scenarios for three future time periods (2030, 2050 and 2070), using the 'Rmaxent' package (i.e. looping 
over each future climate scenario and species, for one period at a time).

\

```{r, eval=FALSE, echo=TRUE}

## A function which projects climatic suitability under six GCMs at each time step (2030/50/70)
env.grids.2030 = tryCatch(project_maxent_grids(scen_list     = scen_2030,
                                               species_list  = map_spp_list,
                                               maxent_path   = maxent_path, 
                                               climate_path  = "./data/base/worldclim/aus/1km/bio",
                                               grid_names    = grid.names,
                                               time_slice    = 30,
                                               current_grids = aus.grids.current),
                          
                          ## Skip species
                          error = function(cond) {
                            
                            message(paste('Species skipped - check', spp))
                            
                          })

```

\

```{r eval = FALSE, echo = TRUE}

## One function for all time periods
project_maxent_grids = function(scen_list, species_list, maxent_path, 
                                climate_path, grid_names, time_slice, current_grids) {
  
  ## Read in Australia
  aus = AUS %>%
    spTransform(ALB.CONICAL)
  
  ## First, run a loop over each scenario:    
  lapply(scen_list, function(x) {
    
    ## Create a raster stack for each of the 6 GCMs, not for each species
    s <- stack(sprintf('%s/20%s/%s/%s%s.tif', climate_path, time_slice, x, x, 1:19))
    projection(s)
    
    ## Rename both the current and future environmental stack...
    names(s) <- names(current_grids) <- grid_names 
    
    #####################################################################
    ## Divide the 11 temperature rasters by 10: NA values are the ocean
    ## s[[1:11]] <- s[[1:11]]/10 ## That code doesn't work
    message('20', time_slice, ' rasters / 10 ', x)
    for(i in 1:11) {
      
      ## Simple loop
      message(i)
      s[[i]] <- s[[ i]]/10
      
    }
    
    ## Then apply each GCM to each species
    lapply(species_list, function(species) {
      
      ## First, check if the maxent model exists
      save_name = gsub(' ', '_', species)
      if(file.exists(sprintf('%s/%s/full/maxent_fitted.rds', maxent_path, species))) {
        message('Doing ', species)
        
        ## Then, check if the species projection has already been run...
        if(!file.exists(sprintf('%s/%s/full/%s_%s.tif', maxent_path, species, species, x))) {
          
          ## Assign the scenario name (to use later in the plot)
          scen_name = eval(parse(text = sprintf('gcms.%s$GCM[gcms.%s$id == x]', time_slice, time_slice)))           
          
          #####################################################################
          ## Now read in the SDM model calibrated on current conditions
          m <- readRDS(sprintf('%s/%s/full/maxent_fitted.rds', maxent_path, species)) 
          m <- m$me_full  ##
          
          ## Read in the occurrence points used to create the SDM :: need the transform to plot later
          occ <- readRDS(sprintf('%s/%s/%s_occ.rds', maxent_path, species, save_name)) %>%
            spTransform(ALB.CONICAL)  
          
          ## Create file path fpr current raster doesn't exist, create it
          f_current <- sprintf('%s/%s/full/%s_current.tif', maxent_path, species, species)
          
          if(!file.exists(f_current)) {
            
            ## Report which prediction is in progress :: m$me_full, m$me_full@presence
            message('Running current prediction for ', species) 
            
            pred.current <- rmaxent::project(
              m, current_grids[[colnames(m@presence)]])$prediction_logistic
            writeRaster(pred.current, f_current, overwrite = TRUE)
            
          } else {
            
            pred.current = raster(sprintf('%s/%s/full/%s_current.tif',
                                          maxent_path, species, species))
          }
          
          #####################################################################
          ## Create file path for future raster doesn't exist, create it 
          f_future <- sprintf('%s/%s/full/%s_%s.tif', 
                              maxent_path, species, species, x)
          
          if(!file.exists(f_future)) {
            
            ## Report which prediction is in progress
            message('Running future prediction for ', species, ' ', x) 
            
            ## Create the future raster
            pred.future <- rmaxent::project(
              m, s[[colnames(m@presence)]])$prediction_logistic
            writeRaster(pred.future, f_future, overwrite = TRUE)
            
            ## Now create the empty panel just before plotting
            empty_ras <- init(pred.current, function(x) NA) 
            
            projection(aus);projection(occ);projection(empty_ras)
            projection(pred.current);projection(pred.future)
            
            identical(extent(pred.current), extent(pred.future))
            
            #####################################################################
            ## Use the levelplot function to make a multipanel output: 
            ## occurrence points, current raster and future raster
            png(sprintf('%s/%s/full/%s_%s.png', maxent_path, species, species, x),      
                11, 4, units = 'in', res = 300)
            
            ## Need an empty frame
            print(levelplot(stack(empty_ras,
                                  pred.current, 
                                  pred.future, quick = TRUE), margin = FALSE,
                            
                            ## Create a colour scheme using colbrewer
                            scales      = list(draw = FALSE), 
                            at = seq(0, 1, length = 100),
                            col.regions = colorRampPalette(rev(brewer.pal(11, 'Spectral'))),
                            
                            ## Give each plot a name: the third panel is the GCM
                            names.attr = c('Australian records', 'Current', 
                                           sprintf('%s, 20%s, RCP8.5', scen_name, time_slice)),
                            colorkey   = list(height = 0.5, width = 3), xlab = '', ylab = '',
                            main       = list(gsub('_', ' ', species), font = 4, cex = 2)) +
                    
                    ## Plot the Aus shapefile with the occurrence points for reference
                    latticeExtra::layer(sp.polygons(aus), data = list(aus = aus)) +
                    latticeExtra::layer(sp.points(occ, pch = 19, cex = 0.15, 
                                                  col = c('red', 'transparent', 'transparent')
                                                  [panel.number()]), data = list(occ = occ)))
            dev.off()
            
          }
          
        } else {
          
          message(species, ' ', x, ' skipped - prediction already run') 
          
        }
        
      } else {
        
        message(species, ' ', x, ' skipped - SDM not yet run') 
        
      }
      
    })
    
  })
  
}

```

\

The resulting maps generated by this function illustrate how climatic suitability varies across the landscape, 
with values of grid cells ranging from 0 (not suitable) to 1 (suitable, see Elith et al. 2011; Merow et al. 2013 
for more details of the  Maxent algorithm).

\newpage 
![](output/figures/CV_figs/A_implexa_CURRENT_SUIT.png)

**Figure 2.** Example of a continuous suitabiliy map for one species under current condtions. 
Species occurrence points are plotted in red on the left panel. The cells in the centre and 
lower panels are coded from 0 - no to low suitability, to 1, highly suitable.

\


#########################################################################################################
## **Summarising the climatic suitability output within Significant Urban areas** 
For each species, we generated maps illustrating areas where suitable climate is projected to occur under 
current and future conditions. In these maps, a grid cell is given a value between 0 (highly unsuitable) 
and 1 (highly suitable). Using a species-specific threshold  -  the 10th percentile training presence 
logistic threshold  -  based on the weighting of different model errors ('commission' errors where 
a grid cell is classified as suitable when it is not, versus 'omission' errors where a grid cell 
is classified as unsuitable when it is suitable) these continuous suitability maps were converted 
into binary suitable/unsuitable maps (0 or 1). This resulted in six maps per species for each of 
the three future time periods - one map for each of the climate scenarios. The six maps were overlaid 
and summed such that the value of a grid cell could range from 0 (unsuitable in all climate scenarios) 
to 6 (suitable in all climate scenarios). Finally, maps were re-coded to quantify the number of grid 
cells that were classified as suitable in a least four of the six climate scenarios. These are grid 
cells that we have greater confidence will be climatically suitability for that species in the future.

\

We then calculated changes to the size of suitable climate in terms of i) overall change in size, 
ii) loss of currently suitable areas and iii) gain in new areas. Finally, we intersected species' maps 
with Significant Urban Areas (SUAs) from the 2016 Census. We then calculated the current area that 
is projected to be climatically suitable within all SUAs, and the extent to which this area may change 
under the future time periods (2030, 2050 and 2070). This was achieved by creating a third function, 
that counts cells lost or gained within an SUA (i.e. looping over each species and maxent threshold, 
for each time period, E.G. 2030)

\

```{r eval = FALSE, echo = TRUE}

#########################################################################################################
## A function which counts the number cells gained or lost for each species inside an SUA
suitability.2030 = tryCatch(mapply(SUA_cell_count,                                               
                                   DIR_list     = SDM.RESULTS.DIR,
                                   species_list = map_spp,
                                   maxent_path  = maxent_path,
                                   thresholds   = percent.10.log,
                                   percentiles  = percent.10.om,
                                   time_slice   = 30,
                                   write_rasters = FALSE),
                            
                            error = function(cond) {
                              
                              message(paste('Species skipped - check inputs', spp))
                              
                            })

```

\

```{r eval = FALSE, echo = TRUE}

########################################################################
## Loop over directories, species and one threshold for each species
SUA_cell_count = function(DIR_list, species_list, maxent_path, thresholds, 
                          percentiles, time_slice, write_rasters) {
  
  ######################################################################
  ## Read in shapefiles :: this should be done outside the loop
  aus        = readRDS('data/base/CONTEXTUAL/aus_states.rds') %>%
    spTransform(ALB.CONICAL)
  
  LAND       = readRDS('data/base/CONTEXTUAL/LAND_world.rds')
  
  areal_unit = readRDS("./data/base/CONTEXTUAL/SUA/SUA_2016_AUST.rds") %>%
    spTransform(ALB.CONICAL)
  areal_unit = areal_unit[order(areal_unit$SUA_NAME16),]
  
  #######################################################################
  ## Read in rasters of the SUA shapefile
  areal_unit_rast = readRDS("./data/base/CONTEXTUAL/SUA/SUA_2016_RAST.rds")
  areal_unit_vec  = readRDS("./data/base/CONTEXTUAL/SUA/SUA_2016_VEC.rds")
  summary(areal_unit_vec)
  
  ## Loop over each directory
  lapply(DIR_list, function(DIR) { 
    
    ## And each species 
    lapply(species_list, function(species) {
      
      ####################################################################
      ## Create a list of the rasters in each directory, then take the mean. T
      message('Running summary of SDM predictions within SUAs for ', species, 
              ' using ', names(areal_unit)[1], " shapefile")
      message('Calcualting mean of 20', time_slice, ' GCMs for ', species)
      
      ## Check if the mean GCM raster exists
      f_mean = sprintf('%s/%s/full/%s_20%s_suitability_mean.tif', 
                       maxent_path, species, species, time_slice)
      
      ## The mean of the GCMs doesn't exist, create it
      if(!file.exists(f_mean)) { 
        
        raster.list       = list.files(as.character(DIR), 
                                       pattern = sprintf('bi%s.tif$', 
                                                         time_slice), full.names = TRUE)  
        suit              = stack(raster.list)
        suit.list         = unstack(suit)
        combo_suit_mean   = mean(suit)          

      } else {
        
        ## Create another level without the mean calculation
        raster.list = list.files(as.character(DIR), 
                                 pattern = sprintf('bi%s.tif$', time_slice), full.names = TRUE)  
        suit        = stack(raster.list)
        suit.list   = unstack(suit)
        
      }
      
      ######################################################################
      ## Then, create rasters that meet habitat suitability criteria thresholds
      for (thresh in thresholds) {
        
        ## Check if the SUA summary table exists
        SUA_file =   sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                             species, species, time_slice, "SUA_cell_count_", thresh)
        
        ## The mean of the GCMs doesn't exist, create it
        if(file.exists(SUA_file)) { 
          
          message(species, ' SUA_table already exists, skip')
          next
          
        }
        
        for (percent in percentiles) {
          
          ## Print the species being analysed
          message('doing ', species, ' | Logistic > ', thresh, ' for 20', time_slice)
          
          ## Read in the current suitability raster
          f_current <- raster(sprintf('%s/%s/full/%s_current.tif', 
                                      maxent_path, species, species))
          
          ## First, create a simple function to threshold each of the rasters in raster.list
          thresh_greater  = function (x) {x > thresh}
          percent_greater = function (x) {x > percent}
          
          ## Then apply this to just the current suitability raster. 
          ## Maximum training sensitivity plus specificity Logistic threshold
          ## 10th percentile training presence training omission
          current_suit_thresh  = thresh_greater(f_current)
          current_suit_percent = percent_greater(f_current) 
          
  
          ################################################################################
          ## First, calculate the cells which are greater that the: 
          ## Maximum training sensitivity plus specificity Logistic threshold
          message('Running thresholds for ', species, ' | 20', 
                  time_slice, ' combined suitability > ', thresh)
          
          suit_ras1_thresh   = thresh_greater(suit.list[[1]])   
          suit_ras2_thresh   = thresh_greater(suit.list[[2]])
          suit_ras3_thresh   = thresh_greater(suit.list[[3]])
          suit_ras4_thresh   = thresh_greater(suit.list[[4]])   
          suit_ras5_thresh   = thresh_greater(suit.list[[5]])
          suit_ras6_thresh   = thresh_greater(suit.list[[6]])
          
          ## Then calculate the cells which are greater than the 
          ## 10th percentile training presence training omission
          suit_ras1_percent  = percent_greater(suit.list[[1]])
          suit_ras2_percent  = percent_greater(suit.list[[2]])
          suit_ras3_percent  = percent_greater(suit.list[[3]])
          suit_ras4_percent  = percent_greater(suit.list[[4]])
          suit_ras5_percent  = percent_greater(suit.list[[5]])
          suit_ras6_percent  = percent_greater(suit.list[[6]])
          
          #################################################################################
          ## Then sum them up: All the threshholds
          combo_suit_thresh   =  Reduce("+", list(suit_ras1_thresh, suit_ras2_thresh, suit_ras3_thresh,
                                                  suit_ras4_thresh, suit_ras5_thresh, suit_ras6_thresh))
          
          ## All the percentiles
          combo_suit_percent  =  Reduce("+", list(suit_ras1_percent, suit_ras2_percent, suit_ras3_percent,
                                                  suit_ras4_percent, suit_ras5_percent, suit_ras6_percent))
          
          #################################################################################
          ## For each species, create a binary raster with cells > 4 GCMs above the maxent 
          ## threshold = 1, and cells with < 4 GCMs = 0. 
          message('Calculating change for ', species, ' | 20', time_slice, ' combined suitability > ', thresh)
          
          ## Functions for thresholding rasters
          band_4           <- function(x) {ifelse(x >=  4, 1, 0) }
          combo_suit_4GCM  <- calc(combo_suit_thresh, fun = band_4)
          
          ##################################################################################
          ## Now create a raster of the gain, loss and stable
          ## Create a raster stack of the current and future rasters
          message ("Counting cells lost/gained/stable/never suitable per SUA")
          
          ## Create a table of cell counts using a raster stack of current and future data
          d <- as.data.frame(stack(current_suit_thresh, combo_suit_4GCM)[]) %>% 
            setNames(c('current', 'future')) %>% 
            mutate(SUA_CODE16 = areal_unit_vec,
                   cell_number = seq_len(ncell(current_suit_thresh))) %>% 
            as.tbl
          dim(d);summary(d)
          
          ## Then classify the cells of the raster stack into lost, gained, stable and never
          d2 <- d %>% 
            na.omit %>% 
            
            mutate(lost   = current == 1 & future == 0,
                   gained = current == 0 & future == 1,
                   stable = current == 1 & future == 1,
                   never  = current == 0 & future == 0,
                   nodata = is.na(current) | is.na(future)) 
          d2$class <- apply(select(d2, lost:never), 1, which)
          dim(d2)
          
          ## Then group the cell counts by SUA
          d3 <- d2 %>% 
            group_by(SUA_CODE16) %>%
            
            summarize(CURRENT_SUITABLE = sum(current, na.rm = TRUE),
                      FUTURE_SUITABLE  = sum(future,  na.rm = TRUE),
                      LOST             = sum(lost,    na.rm = TRUE),
                      GAINED           = sum(gained,  na.rm = TRUE),
                      STABLE           = sum(stable,  na.rm = TRUE),
                      NEVER            = sum(never,   na.rm = TRUE),
                      NODAT            = sum(nodata,  na.rm = TRUE),
                      n_cells = n()) %>% 
            
            ## Then calculate change between current and future
            mutate(CHANGE    = FUTURE_SUITABLE - CURRENT_SUITABLE,
                   GAIN_LOSS = ifelse(CHANGE < 0, 'LOSS', ifelse(CHANGE > 0, 'GAIN', 'STABLE')),
                   GAIN_LOSS = ifelse(CURRENT_SUITABLE == 0 & FUTURE_SUITABLE == 0, 'NEVER', GAIN_LOSS))
          dim(d3)
          
          ## Add the species column
          d4 = d3 %>% 
            join(areal_unit@data, .) %>%
            add_column(., SPECIES = species,    .after = "AREASQKM16") %>%
            add_column(., PERIOD  = time_slice, .after = "SPECIES")    %>%
            add_column(., THRESH  = thresh,     .after = "PERIOD")
          View(d4)
          
          ##################################################################################
          ## Now calculate the number of cells lost/gained/stable across Australia
          message ("Counting cells lost/gained/stable/never suitable across Australia")
          d5 <- stack(current_suit_thresh, combo_suit_4GCM)[]
          r <- raster(current_suit_thresh)
          z <- as.data.frame(d4)
          
          ## Then classify the raster stack to make each value (i.e. outcome) unique
          r[d5[, 1]==1 & d5[, 2]==0] <- 1  ## 1 in current raster and 0 in future = LOSS
          r[d5[, 1]==0 & d5[, 2]==1] <- 2  ## 0 in current raster and 1 in future = GAIN
          r[d5[, 1]==1 & d5[, 2]==1] <- 3  ## 1 in current raster and 1 in future = STABLE
          r[d5[, 1]==0 & d5[, 2]==0] <- 4  ## 0 in current raster and 0 in future = NEVER_SUIT
          
          ## Now convert the raster to a factor and assign lables to the levels
          gain_loss <- as.factor(r)
          levels(gain_loss)[[1]] <- data.frame(ID = 1:4, 
                                               label = c('Lost', 'Gained', 'Stable', 'Never_Suitable'))
          z <- as.data.frame(d5)
          
          ## The gain/loss raster could be intersected with the SUAs
          gain_loss_table      = table(z[, 1], z[, 2])
          gain_loss_df         = as.data.frame(raster::freq(gain_loss))
          gain_loss_df$SPECIES = species
          gain_loss_df$PERIOD  = time_slice
          
          names(gain_loss_df)  = c("CHANGE", "COUNT", "SPECIES", "PERIOD")
          gain_loss_df         = gain_loss_df[, c("SPECIES", "PERIOD", "CHANGE", "COUNT")]
          
          ## Change values and remove the NA row
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 1] <- "LOST"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 2] <- "GAINED"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 3] <- "STABLE"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 4] <- "NEVER_SUIT"
          gain_loss_df = head(gain_loss_df, 4)
          head(gain_loss_df)
          
          ##################################################################################
          ## Save the continental gain/loss table
          write.csv(gain_loss_df, sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                                          species, species, time_slice, "gain_loss_table_", thresh), 
                    row.names = FALSE)
          
          ##################################################################################
          ## Save the SUA gain/loss table
          write.csv(d4, sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                                species, species, time_slice, "SUA_cell_count_", thresh), 
                    row.names = FALSE)
          
          ##################################################################################
          ## Now write the rasters
          ## If the rasters don't exist, write them for each species/threshold
          if(write_rasters == "TRUE") {
            
            ## Write the combined future raster with > 4 GCMs above the maximum training value
            message('Writing ', species, ' | 20', time_slice, ' 4 GCMs > ', percent)
            writeRaster(combo_suit_4GCM, sprintf('%s/%s/full/%s_20%s%s%s.tif', maxent_path,
                                                 species, species, time_slice, 
                                                 "_4GCMs_above_", thresh), overwrite = TRUE)
            
            ## Write out the gain/loss raster
            writeRaster(gain_loss, sprintf('%s/%s/full/%s_20%s%s%s.tif', maxent_path,
                                           species, species, time_slice, "_gain_loss_", thresh), 
                        datatype = 'INT2U', overwrite = TRUE)

          } else {
            
            message(' skip raster writing') 
            
          }
          
        }
        
      }
      
    })
    
  })
  
}

```


\newpage 
![](output/figures/CV_figs/A_implexa_FUTURE_SUIT.png)

**Figure 3.** Example of a combined suitabiliy map for one species under six climate models for 2030. 
Species occurrence points are plotted in red on the left panel. The cells in the left and lower panels 
are coded as cells where the species is predicted to be lost, gained, or remain stable.

\

#########################################################################################################
# **<span style="color:blue"> Results </span>**
#########################################################################################################

\

Generally, the Climatic suitability analyses predict that species in SUAs in cooler areas (e.g., Hobart) 
are projected to experience greater expansion of suitable climate compared to species in SUAs in warmer 
areas (e.g. Darwin). This increase of suitable habitat for more species in cooler SUAs is driven by the 
projection of a southwards shift in climate space: species northern-most range margins are projected to 
contract, while their southern-most margins may be extended into new areas. Although there is no clear 
climatic pattern in species losses from SUAs, our projections indicate that the warmer SUAs generally 
lose more species than they gain. For example, of the eight most populous SUAs in each Australian State 
and Territory (Hobart, Canberra, Melbourne, Adelaide, Sydney, Perth, Brisbane and Darwin) the warmest 
will lose suitable climate for more species than they will gain.


\newpage 
![](output/figures/CV_figs/ALL_SUA.png)

**Figure 4.** The left panel shows a barplot of predicted percentage of original native tree species 
gains (green) and losses (red) within all of Australia’s 101 significant Urban Areas (SUAs), from 
current distributions to 2070, derived from the climatic suitability models (CSMs). SUAs are ranked 
by current mean annual temperature (MAT, e.g. Hobart is the coolest SUA, Darwin the the warmest). 
The top right panel shows a scatterplot of the species gained in each SUA (y) vs the MAT of each SUA,
while the bottom right panel shows the losses.


\newpage 
![](output/figures/CV_figs/BIG_SUA.png)

**Figure 5.** The left panel shows a barplot of predicted percentage of original native tree species 
gains (green) and losses (red) within all Australia’s largest 21 significant Urban Areas (SUAs), from 
current distributions to 2070, derived from the climatic suitability models (CSMs). SUAs are ranked 
by current mean annual temperature (MAT, e.g. Hobart is the coolest SUA, Darwin the the warmest). 
The top right panel shows a scatterplot of the species gained in each SUA (y) vs the MAT of each SUA,
while the bottom right panel shows the losses.

\

Our preliminary results indicate that climatically suitable habitat for many species is likely to decline 
For the 176 species analysed here, suitable habitat for half these species is projected to decline 
by 2070 to less than 42% of its current extent. Only 4.5% of species are projected to have increases 
to the size of suitable habitat. 

\

#########################################################################################################
# **<span style="color:blue"> Implications, caveats and future work </span>**
#########################################################################################################

\

The repercussions of loss and gain for the Nursery Industry is two fold. Firstly, if an area becomes unsuitable 
for a species, this means that existing plants are likely vulnerable to climate change. Secondly, it signifies 
changing opportunities for growers: species that may have been solid performers in the past may be unreliable 
in the future, while new opportunities will emerge as suitable climate space appears beyond a species’ current 
range. Generally, climate space is projected to shift southwards: species northern-most range margins are 
projected to contract, while southern-most margins may be extended into new places. Our projections indicate, 
however, that the five most populous SUAs in Australia (Adelaide, Brisbane, Melbourne, Perth and Sydney) will 
lose suitable climate for more species than they will gain - at least for the current crop of 176 species 
included in this analysis. 

\

While CSMs are useful tools for exploring the distribution of climatically suitable habitat, several factors 
should be kept in mind when interpreting their output - particularly when applied to plants growing in urban 
environments.

\

**1).** CSMs do not ‘predict’ where a species will occur. These tools identify where suitable conditions for 
the species occur with respect to the climatic variables used to calibrate the model. However, climatic 
extremes have been excluded from the model, which may alter the likelihood that a plant will survive in 
a given location.

\

**2).**  The models were generated at a spatial resolution of 1 x 1 km. At a finer scale, microclimatic 
characteristics also need to be considered when determining the suitability of a location for a species.

\

**3).**  CSMs provide an indication of the exposure of a species to climate change. However, they do not account 
for plasticity in the response of individuals to weather and climate. A species may be able to tolerate 
a broader range of conditions than is apparent from the distribution of its occurrence records. As such, 
the species may be able to survive in areas projected to be unsuitable.

\

In sum, CSMs are tools for developing a broad understanding of responses to climate change and are best 
used when combined with species-specific trait or experimental data.

\

#########################################################################################################
# **TBC...**
#########################################################################################################