---
title: "Predicted change of suitable habitat for horticultural trees in urban areas under climate change"
authors: "Hugh Burley, John Baumgartner, Linda Beaumont"
date: "December 2018"
output:
html_document: default
pdf_document: default
---

\

This R markdown file summarises my contribution to the 'Which Plant Where' project at Macquarie University, 
outlining the key functions I have created during my postdoctural work. WPW is part of the Green Cities
co-investment fund : 

https://horticulture.com.au/co-investment-fund/green-cities-fund/


https://www.whichplantwhere.com.au/ 


The text and code below summarises a workflow in R that can be used to relatively rapidly assess the effect
of climate change on a given taxon within Australia, from dowloading occurrence records, through to creating
maps of predicted climatic suitablity across Australia at 1km*1km resolution. An example for 10 species is
given in the markdown file.

\


#########################################################################################################
# **<span style="color:blue"> Summary </span>**
#########################################################################################################

\

**Aim:** Globally, local government authorities are increasing their investment in urban greening 
interventions, yet there is little consideration of whether the current palette of species for these 
plantings will be resilient to climate change. We assessed the distribution of climatically suitable 
habitat, now and in the future, for 176 of the tree species most commonly grown by nurseries and 
planted across Australia's urban landscapes. 

**Location:** Australian Significant Urban Areas 

**Time period:** 1960 - 2070

**Major taxa analysed:** 176 commonly planted and sold horticultural tree species

**Methods:** Species’ occurrence records were obtained from myriad inventories and herbaria, 
both globally and across Australia, along with baseline climate data (WorldClim, 1960-1990) 
and six climate scenarios for 2030 and 2070. Climatic suitability models (CSMs) for each 
species were calibrated and projected onto baseline and future climate scenarios. We 
calculated changes to the size of climatically suitable habitat across SUAs, for each 
species, and identified urban areas that are likely to have suitable climate for fewer 
or more of our study species as climate changes.

**Results:** By 2070, climatically suitable habitat in urban regions is projected 
to decline for ~73% of species, while for 18% of species, suitable climate is predicted 
to decline to less than half of its baseline extent. Generally, urban areas in cooler 
regions are predicted to gain more species than they lose, while urban areas in warmer 
regions may lose more species than they gain. Our results highlight changing patterns 
of urban climatic space for different species, indicating that governments and nurseries 
should take a proactive approach –  utilizing multiple lines of evidence (modelling and 
experiments) – to identify new planting opportunities. 


\


#########################################################################################################
# **<span style="color:blue"> Methods </span>**
#########################################################################################################

#########################################################################################################
### **Defining significant Urban Areas** 
Our spatial units for estimating species' future climatic suitability are the 101 Australian Significant 
Urban Areas (SUAs). SUAs are defined by the Australian Bureau of Statistics (ABS) as containing at least 
10,000 people within a single labour market. The SUA boundaries were taken from the ABS 2016 SUA shapefile, 
and cover the full span of current Australian climate (Fig 1). 


\newpage 
![](output/figures/CV_figs/SUA_TEMP_ESA_PLOT.png)


\newpage 
![](output/figures/CV_figs/figMAT_PET_SUA.png)

**Figure 1.** Largest Significant Urban Areas in each Australian State and Territory 
(top panel). The distribution of SUAs in climate space [bottom panel, largest SUAs labelled. 
MAT = current mean annual temperature (worldclim 1960-1990), MAP = current annual 
precipitation, PET = current potential evaporation]. 

\

#########################################################################################################
## **Species selection**
To analyse horticulturally significant Australian species, we obtained a list of native tree species 
grown by the nursery industry across Australia, as well as the number of nurseries growing each species 
within each state or territory. We then created a spatial database of urban tree inventories in Australia 
by contacting local government authorities. This resulted in inventory data for 41 local government areas 
spanning  20 SUAs. The 400 most frequently reported trees within the inventories were then intersected 
with the nursery list (i.e. those trees currently being sold within any Australian nursery), creating 
a list of 248 native species. We consider these 248 species as the most commonly planted and sold native 
trees in the current Australian horticultural market. This species list was first checked against the 
backbone taxonomy of the Global Biodiversity Information Facilty (GBIF, www.gbif.org), and then against 
]The Plant List (TPL) backbone taxonomy using the Taxonstand package 
[version 2.1, Cayuela et al (2012)] in the R language [Version 3.5.1, R Core team (2018)]. The accepted 
names from the TPL taxonomy were then used for this analysis. 

\

```{r message=FALSE, echo=FALSE, warning=FALSE}

########################################################################################################
## List of Variables used to run the SDM analyses
load("H:/green_cities_sdm/TEST_RUN.RData")
source('./R/RMD_READ_PACKAGES.R')

```

\

#########################################################################################################
## **Occurrence data** 
For each of these planted and sold species, we downloaded global occurrence records from GBIF and the 
Atlas of Living Australia (ALA, www.ala.org.au) using the rgbif and ALA4R packages in R 
[https://github.com/AtlasOfLivingAustralia/ALA4R, (Hijmans et al. 2016) (R Core Team, 2017)]. 

\

```{r eval = FALSE, echo = TRUE}

## Download ALA data
download_ALA_all_species = function (species_list, path) {
  
  ## create variables
  skip.spp.list       = list()

  ## for every species in the list
  for(sp.n in species_list){
    
    ## 1). First, check if the f*&%$*# file exists
    file = paste0(path, sp.n, "_ALA_records.RData")
    
    ## If it's already downloaded, skip
    if (file.exists (file)) {
      
      print (paste ("file exists for species", sp.n, "skipping"))
      next
      
    }
    
    ## 2). Then check the spelling...incorrect nomenclature will return NULL result
    if (is.null(occurrences(taxon = sp.n, download_reason_id = 7)$data) == TRUE) {
      
      ## Now append the species which had incorrect nomenclature to the skipped list
      ## this is slow, but it works for now
      print (paste ("Possible incorrect nomenclature", sp.n, "skipping"))
      nomenclature = paste ("Possible incorrect nomenclature |", sp.n)
      skip.spp.list <- c(skip.spp.list, nomenclature)
      next
      
    }
    
    ## 3). Skip species with no records
    if (dim(occurrences(taxon = sp.n, download_reason_id = 7)$data)[1] <= 2) {
      
      ## now append the species which had no records to the skipped list
      print (paste ("No ALA records for", sp.n, "skipping"))
      records = paste ("No ALA records |", sp.n)
      skip.spp.list <- c(skip.spp.list, records)
      next
      
    }
    
    ## 4). Download ALL records from ALA :: 
    message("Downloading ALA records for ", sp.n, " using ALA4R :: occurrences")
    ALA = occurrences(taxon = sp.n, download_reason_id = 7) 
    ALA = ALA[["data"]]
    cat("Synonyms returned for :: ", sp.n, unique(ALA$scientificName), sep="\n")
    message(dim(ALA[1]), " Records returned for ", sp.n)
    
    ## 5). save records to .Rdata file
    save(ALA, file = paste(path, sp.n, "_ALA_records.RData", sep = ""))
    
  }
  
}

```

\

We then undertook an intensive spatial data-cleaning process to remove spatially invalid or suspect records 
that, if retained, can cause species' climate tolerances to be miscalculated. Spatially invalid records, 
those taken before 1950, duplicate records, those within 10 km of capital cities, herbaria and biodiversity 
institutions, and individual records > 300 km from other records were all removed using the CoordinateCleaner 
package (version 1.0-7). We then calibrated climate suitability models (CSMs) for all 248 native planted 
and sold tree species using best practices, as summarised below.

\

```{r echo = FALSE, message = FALSE, warning = FALSE}
## Step 3 :: combine GBIF occurrence data with ALA data and filter to records > 1950
source('./R/ALA_DATA_FILTER_TAXO_SCIENTIFIC_NAME.R', echo = FALSE)
source('./R/3_GBIF_DATA_TAXO_SCIENTIFIC_NAME.R',     echo = FALSE)


## Step 4 :: combine GBIF, ALA and tree inventory data into a single table, extract environmental condtions
source('./R/4_ALA_GBIF_URBAN_COMBINE.R',  echo = FALSE)
source('./R/INVENTORY_RASTER.R',          echo = FALSE)


## Step 5 :: clean the occurrence data using the 'CleanCoordinates' function in the CoordinateCleaner package to remove
## records near herbaria, duplicates, etc. & add contextual info for each record (taxonomic and horticultural) 
## Then prepare the SDM table
## Then clean the spatial outliers
source('./R/5_GBIF_ALA_CLEAN_NICHES.R',  echo = FALSE)
source('./R/6_PREPARE_SDM_TABLE_1KM.R',  echo = FALSE)


```

\

#########################################################################################################
## **Climate data**
We obtained baseline climate data from the WorldClim Database [worldclim.org/bioclim, Version 1.4 
(Hijmans et al. 2005)]. WorldClim comprises 19 bioclimatic variables, summarised for the period 1960-1990, 
of which we used eight for model calibration: Annual mean temperature, Temperature seasonality, 
Maximum temperature of the warmest month, Minimum temperature of the coldest month, Annual precipitation, 
Precipitation seasonality, Precipitation of the wettest month, and Precipitation of the driest month. 
These variables were chosen to capture climate averages, seasonality and extremes, all of which have 
been identified as important variables for predicting suitable habitat for plants (Bradie & Leung, 2017). 

\

When assessing the impacts of climate change, it is more robust to use projections from multiple climate 
models (IPCC, 2014; Taylor, Stouffer, & Meehl, 2011; Beaumont et al. 2007, Baumgartner et al 2018). We 
utilised a subset of six models recommended by CSIRO's Climate Change in Australia report 
(https://www.climatechangeinaustralia.gov.au/en/), as these models were shown to perform better than others 
in their ability to simulate historical Australian climate (Table 1). Hence, more confidence can be placed 
in their projections of future climate. For each of these six GCMs, we downloaded monthly maximum and 
minimum temperature and monthly precipitation from CSIRO for 2030, 2050 and 2070, at a spatial resolution 
of 30 arc-seconds (~1 km). From these data, we calculated the eight bioclimatic variables for the three 
time periods and re-projected the data to an equal area grid of 1 km x 1 km resolution to match the baseline 
climate, using R [version 3.4.2, R Core Team (2017)]. 

\

#########################################################################################################
## **Modeling approach**
We modelled climatic suitability under current conditions (1960-1990) using the Maxent algorithm 
(Elith et al. 2011; Phillips et al. 2006; Phillips and Dudik 2008) within the Dismo R package 
(Hijmans et al. 2016). Maxent is a machine learning, correlative approach to modelling climatic suitability 
that is generally regarded as superior to other algorithms that are used to model presence-only 
occurrence data (Elith et al. 2006). To run models for all species, we created a function in the 
R programming language which splits a table of species records * environment into only those spatial 
records for a given species, and calibrates a Maxent model under current climate for that species.

\


```{r run maxent, eval = FALSE, echo = TRUE}

#########################################################################################################
## A function which splits a table of species records * enviromental conditions into records for each 
## species, then fits a maxent model under current climatic conditions.
lapply(GBIF.spp, function(spp){ 
  
  ## Skip the species if the directory already exists, before the loop
  outdir <- maxent_dir
  
  if(dir.exists(file.path(maxent_path, gsub(' ', '_', spp)))) {
    message('Skipping ', spp, ' - already run.')
    invisible(return(NULL))
    
  }
  
  ## Print the taxa being processed to screen
  if(spp %in% SDM.SPAT.ALL$searchTaxon) {
    message('Doing ', spp) 
    
    ## Subset the records to only the taxa being processed
    occurrence <- subset(SDM.SPAT.ALL, searchTaxon == spp)
    
    ## Now get the background points. These can come from any spp, other than the modelled species.
    background <- subset(SDM.SPAT.ALL, searchTaxon != spp)
    
    ## Finally fit the models using FIT_MAXENT_TARG_BG. 
    tryCatch(
      FIT_MAXENT_TARG_BG(occ                     = occurrence,  ## occurrence points
                         bg                      = background,  ## background points
                         sdm.predictors          = sdm.select,  ## predictor variables
                         name                    = spp,         ## Species name
                         outdir,                                ## Output directory
                         template.raster,                       ## Resolution raster
                         min_n                   = 20,          ## Minimum records           
                         max_bg_size             = 70000,       ## Max bg points
                         Koppen                  = Koppen_1975, ## Koppen Shapefile 
                         background_buffer_width = 200000,      ## Km
                         shapefiles              = TRUE,        ## Save bg and occ shp?
                         features                = 'lpq',       ## Maxent features
                         replicates              = 5,           ## No. of K-fold
                         responsecurves          = TRUE),       ## save response curves?
      
      error = function(cond) {

        message(paste('Species skipped ', spp))

      })
    
  } else {
    
    message(spp, ' skipped - no data.')   
    
  }  
  
})


```

\


```{r eval = FALSE, echo = TRUE}

## Print the maxent function
FIT_MAXENT_TARG_BG <- function(occ,
                               bg, # A Spatial points data frame (SPDF) of candidate background points
                               sdm.predictors, # a vector of enviro conditions that you want to include
                               name,
                               outdir,
                               template.raster,
                               # template.raster is an empty raster with extent, 
                               # res and projection of final output rasters. 
                               # It is used to reduce occurrences to a single 
                               # point per cell.
                               min_n,
                               # min_n is the minimum number of records (unique cells)
                               # required for a model to be fit
                               max_bg_size,
                               background_buffer_width, # How many km to buffer
                               shapefiles,
                               features,
                               replicates, # number of cross-validation replicates
                               responsecurves,
                               rep_args,
                               full_args) {
  
  ########################################################################
  ## First, stop if the outdir file exists,
  if(!file.exists(outdir)) stop('outdir does not exist :(', call. = FALSE)
  outdir_sp <- file.path(outdir, gsub(' ', '_', name))
  
  if(!missing('Koppen')) {
    if(!is(Koppen, 'RasterLayer'))
      stop('Koppen must be a RasterLayer, should be the same coord system as template.raster')  
  }
  
  ## If the file doesn't exist, split out the features
  if(!file.exists(outdir_sp)) dir.create(outdir_sp)
  features <- unlist(strsplit(features, ''))
  
  ## Make sure user features are allowed: don't run the model if the
  ## features have been incorrectly specified in the main argument
  ## l: linear
  ## p: product
  ## q: quadratic
  ## h: hinge
  ## t: threshold
  if(length(setdiff(features, c('l', 'p', 'q', 'h', 't'))) > 1)
    stop("features must be a vector of one or more of ',
         'l', 'p', 'q', 'h', and 't'.")
  
  ## Aggregate
  b <- aggregate(gBuffer(occ, width = background_buffer_width, byid = TRUE))
  
  #####################################################################
  ## Get unique cell numbers for species occurrences
  cells <- cellFromXY(template.raster, occ)
  
  ## Clean out duplicate cells and NAs 
  ## Note this will get rid of a lot of duplicate records 
  not_dupes <- which(!duplicated(cells) & !is.na(cells))
  occ       <- occ[not_dupes, ]
  cells     <- cells[not_dupes]
  message(nrow(occ), ' occurrence records (unique cells).')
  
  #####################################################################
  ## Skip species that have less than a minimum number of records: eg 20 species
  if(nrow(occ) < min_n) {
    
    print (paste ('Fewer occurrence records than the number of cross-validation ',
                  'replicates for species ', name,
                  ' Model not fit for this species'))
    
  } else {
    
    ## Subset the background records to the 200km buffered polygon
    message(name, ' creating background cells')
    system.time(o <- over(bg, b))
    bg <- bg[which(!is.na(o)), ]
    bg_cells <- cellFromXY(template.raster, bg)
    
    ## Clean out duplicates and NAs (including points outside extent of predictor data)
    bg_not_dupes <- which(!duplicated(bg_cells) & !is.na(bg_cells))
    bg <- bg[bg_not_dupes, ]
    bg_cells <- bg_cells[bg_not_dupes]
    
    ## Find which of these cells fall within the Koppen-Geiger zones that the species occupies
    if(!missing('Koppen')) {
      
      ## Crop the Kopppen raster to the extent of the occurrences, and snap it
      message(name, ' intersecting background cells with Koppen zones')
      Koppen_crop <- crop(Koppen, occ, snap = 'out')
      
      ## Only extract and match those cells that overlap between koppen_cropp, occ and bg 
      zones               <- raster::extract(Koppen_crop, occ)
      cells_in_zones_crop <- Which(Koppen_crop %in% zones, cells = TRUE)
      cells_in_zones      <- cellFromXY(Koppen, xyFromCell(Koppen_crop, cells_in_zones_crop))
      bg_cells            <- intersect(bg_cells, cells_in_zones)
      i                   <- cellFromXY(template.raster, bg)
      bg                  <- bg[which(i %in% bg_cells), ]
      
    }
    
    ## Reduce background sample if it's larger than max_bg_size
    if (nrow(bg) > max_bg_size) {
      
      message(nrow(bg), ' target species background records, reduced to random ',
              max_bg_size, '.')
      
      bg <- bg[sample(nrow(bg), max_bg_size), ]  ## Change this to use 
      
    } else {
      
      message(nrow(bg), ' target species background records.')
      
    }
    
    #####################################################################
    ## Save occ and bg shapefiles objects for future reference
    save_name = gsub(' ', '_', name)
    if(shapefiles) {
      
      suppressWarnings({
        
        message(name, ' writing occ and bg shapefiles')
        writeOGR(SpatialPolygonsDataFrame(b, data.frame(ID = seq_len(length(b)))),
                 outdir_sp, paste0(save_name, '_bg_buffer'), 'ESRI Shapefile', overwrite_layer = TRUE)
        writeOGR(bg,  outdir_sp, paste0(save_name, '_bg'),   'ESRI Shapefile', overwrite_layer = TRUE)
        writeOGR(occ, outdir_sp, paste0(save_name, '_occ'),  'ESRI Shapefile', overwrite_layer = TRUE)
        
      })
      
    }
    
    ## Save the background and occurrence points as objects
    saveRDS(bg,  file.path(outdir_sp, paste0(save_name, '_bg.rds')))
    saveRDS(occ, file.path(outdir_sp, paste0(save_name, '_occ.rds')))
    
    #####################################################################
    swd_occ <- occ[, sdm.predictors]
    saveRDS(swd_occ, file.path(outdir_sp, paste0(save_name,'_occ_swd.rds')))
    
    swd_bg <- bg[, sdm.predictors]
    saveRDS(swd_bg, file.path(outdir_sp, paste0(save_name, '_bg_swd.rds')))
    
    ## Save shapefiles of the occurrence and background points
    if(shapefiles) {
      
      writeOGR(swd_occ, outdir_sp,  paste0(save_name, '_occ_swd'), 
               'ESRI Shapefile', overwrite_layer = TRUE)
      
      writeOGR(swd_bg,  outdir_sp,  paste0(save_name, '_bg_swd'),  
               'ESRI Shapefile', overwrite_layer = TRUE)
      
    }
    
    #####################################################################
    ## Combine occurrence and background data
    swd <- as.data.frame(rbind(swd_occ@data, swd_bg@data))
    saveRDS(swd, file.path(outdir_sp, 'swd.rds'))
    pa <- rep(1:0, c(nrow(swd_occ), nrow(swd_bg)))
    
    ## Now check the features arguments are correct
    off <- setdiff(c('l', 'p', 'q', 't', 'h'), features)
    
    ## 
    if(length(off) > 0) {
      
      off <- c(l = 'linear=false',    p = 'product=false', q = 'quadratic=false',
               t = 'threshold=false', h = 'hinge=false')[off]
      
    }
    
    off <- unname(off)
    
    if(replicates > 1) {
      
      if(missing(rep_args)) rep_args <- NULL
      
      ## Run MAXENT for x cross validation data splits of swd : so 5 replicaes, 0-4
      message(name, ' running xval maxent')
      me_xval <- maxent(swd, pa, path = file.path(outdir_sp, 'xval'),
                        args = c(paste0('replicates=', replicates),
                                 'responsecurves=true',
                                 'outputformat=logistic',
                                 off, paste(names(rep_args), rep_args, sep = '=')))
      
    }
    
    ## Runs the full maxent model - using all the data in swd
    ## This uses DISMO to output standard files, but the names can't be altered
    if(missing(full_args)) full_args <- NULL
    message(name, ' running full maxent')
    me_full <- maxent(swd, pa, path = file.path(outdir_sp, 'full'),
                      args = c(off, paste(names(full_args), full_args, sep = '='),
                               'responsecurves=true',
                               'outputformat=logistic'))
    
    ## Save the full model
    saveRDS(list(me_xval = me_xval, me_full = me_full, swd = swd, pa = pa, 
                 koppen_gridcode = as.character
                 (Koppen_zones$Koppen[match(unique(zones), Koppen_zones$GRIDCODE)])), 
            file.path(outdir_sp, 'full', 'maxent_fitted.rds'))
    
    #####################################################################
    ## Save the chart corrleation file too for the variable set
    png(sprintf('%s/%s/full/%s_%s.png', outdir,
                save_name, save_name, "predictor_correlation"),
        3236, 2000, units = 'px', res = 300)
    
    ## set margins
    par(mar   = c(3, 3, 5, 3),  ## b, l, t, r
        oma   = c(1.5, 1.5, 1.5, 1.5))
    
    ## Add detail to the response plot
    chart.Correlation(swd_occ@data,
                      histogram = TRUE, pch = 19) 

    ## Finish the device
    dev.off()
    
  }
  
}

```

\

The performance of each model was estimated by calculating the average test Area Under the Receiver 
Operating Characteristic curve (AUC, Swets, 1988) and the True Skill Statistic (TSS) through five-fold 
cross-validation. This involved splitting the occurrence data for each species into five subsets of 
roughly equal size (i.e. folds), fitting the model to four of the five folds and predicting to the 
fifth. This process was repeated until each fold was used four times for model fitting and once for 
model evaluation (Stone, 1974). Below is a plot and table of the results for the full models (i.e. 
using all the data). 

\

```{r maxent table, message = FALSE, echo = FALSE, warning = FALSE}
source('./R/MAXENT_TABLE.R', echo = FALSE)
kable(MAXENT.RESULTS[c("searchTaxon", "X.Background.points", "Var_pimp", "Perm_imp", "Training.AUC",  "max_tss")])

```

\

#########################################################################################################
## **Combining the  models**
We then created a function to take the maxent model for each species, and project it onto the six climate 
scenarios for three future time periods (2030, 2050 and 2070), using the 'Rmaxent' package (i.e. looping 
over each future climate scenario and species, for one period at a time).

\

```{r, eval=FALSE, echo=TRUE}

## A function which projects climatic suitability under six GCMs at each time step (2030/50/70)
env.grids.2030 = tryCatch(project_maxent_grids(scen_list     = scen_2030,    ## List of global circulation models
                                               species_list  = map_spp_list, ## List of species' directories
                                               maxent_path   = maxent_path,  ## Directory of maxent results
                                               climate_path  = "./clim/",    ## Directory of climate rasters
                                               grid_names    = grid.names,   ## Names of climate rasters
                                               current_grids = aus.grids,    ## Stack of climate rasters
                                               time_slice    = 30)           ## Time period 
                          
                          ## Skip species
                          error = function(cond) {
                            
                            message(paste('Species skipped - check', spp))
                            
                          })

```

\

```{r eval = FALSE, echo = TRUE}

## Run the porject function for each time period separately
project_maxent_grids = function(scen_list, species_list, maxent_path, 
                                climate_path, grid_names, time_slice, current_grids) {
  
  ## Read in Australia
  aus = AUS %>%
    spTransform(ALB.CONICAL)
  
  ## First, run a loop over each scenario:    
  lapply(scen_list, function(x) {
    
    ## Create a raster stack for each of the 6 GCMs, not for each species
    s <- stack(sprintf('%s/20%s/%s/%s%s.tif', climate_path, time_slice, x, x, 1:19))
    projection(s)
    
    ## Rename both the current and future environmental stack...
    names(s) <- names(current_grids) <- grid_names 
    
    #####################################################################
    ## Divide the 11 temperature rasters by 10: NA values are the ocean
    ## s[[1:11]] <- s[[1:11]]/10 ## That code doesn't work
    message('20', time_slice, ' rasters / 10 ', x)
    for(i in 1:11) {
      
      ## Simple loop
      message(i)
      s[[i]] <- s[[ i]]/10
      
    }
    
    ## Then apply each GCM to each species
    lapply(species_list, function(species) {
      
      ## First, check if the maxent model exists
      save_name = gsub(' ', '_', species)
      if(file.exists(sprintf('%s/%s/full/maxent_fitted.rds', maxent_path, species))) {
        message('Doing ', species)
        
        ## Then, check if the species projection has already been run...
        if(!file.exists(sprintf('%s/%s/full/%s_%s.tif', maxent_path, species, species, x))) {
          
          ## Assign the scenario name (to use later in the plot)
          scen_name = eval(parse(text = sprintf('gcms.%s$GCM[gcms.%s$id == x]', time_slice, time_slice)))           
          
          #####################################################################
          ## Now read in the SDM model calibrated on current conditions
          m <- readRDS(sprintf('%s/%s/full/maxent_fitted.rds', maxent_path, species)) 
          m <- m$me_full  ##
          
          ## Read in the occurrence points used to create the SDM :: need the transform to plot later
          occ <- readRDS(sprintf('%s/%s/%s_occ.rds', maxent_path, species, save_name)) %>%
            spTransform(ALB.CONICAL)  
          
          ## Create file path fpr current raster doesn't exist, create it
          f_current <- sprintf('%s/%s/full/%s_current.tif', maxent_path, species, species)
          
          if(!file.exists(f_current)) {
            
            ## Report which prediction is in progress :: m$me_full, m$me_full@presence
            message('Running current prediction for ', species) 
            
            pred.current <- rmaxent::project(
              m, current_grids[[colnames(m@presence)]])$prediction_logistic
            writeRaster(pred.current, f_current, overwrite = TRUE)
            
          } else {
            
            pred.current = raster(sprintf('%s/%s/full/%s_current.tif',
                                          maxent_path, species, species))
          }
          
          #####################################################################
          ## Create file path for future raster doesn't exist, create it 
          f_future <- sprintf('%s/%s/full/%s_%s.tif', 
                              maxent_path, species, species, x)
          
          if(!file.exists(f_future)) {
            
            ## Report which prediction is in progress
            message('Running future prediction for ', species, ' ', x) 
            
            ## Create the future raster
            pred.future <- rmaxent::project(
              m, s[[colnames(m@presence)]])$prediction_logistic
            writeRaster(pred.future, f_future, overwrite = TRUE)
            
            ## Now create the empty panel just before plotting
            empty_ras <- init(pred.current, function(x) NA) 
            
            projection(aus);projection(occ);projection(empty_ras)
            projection(pred.current);projection(pred.future)
            
            identical(extent(pred.current), extent(pred.future))
            
            #####################################################################
            ## Use the levelplot function to make a multipanel output: 
            ## occurrence points, current raster and future raster
            png(sprintf('%s/%s/full/%s_%s.png', maxent_path, species, species, x),      
                11, 4, units = 'in', res = 300)
            
            ## Need an empty frame
            print(levelplot(stack(empty_ras,
                                  pred.current, 
                                  pred.future, quick = TRUE), margin = FALSE,
                            
                            ## Create a colour scheme using colbrewer
                            scales      = list(draw = FALSE), 
                            at = seq(0, 1, length = 100),
                            col.regions = colorRampPalette(rev(brewer.pal(11, 'Spectral'))),
                            
                            ## Give each plot a name: the third panel is the GCM
                            names.attr = c('Australian records', 'Current', 
                                           sprintf('%s, 20%s, RCP8.5', scen_name, time_slice)),
                            colorkey   = list(height = 0.5, width = 3), xlab = '', ylab = '',
                            main       = list(gsub('_', ' ', species), font = 4, cex = 2)) +
                    
                    ## Plot the Aus shapefile with the occurrence points for reference
                    latticeExtra::layer(sp.polygons(aus), data = list(aus = aus)) +
                    latticeExtra::layer(sp.points(occ, pch = 19, cex = 0.15, 
                                                  col = c('red', 'transparent', 'transparent')
                                                  [panel.number()]), data = list(occ = occ)))
            dev.off()
            
          }
          
        } else {
          
          message(species, ' ', x, ' skipped - prediction already run') 
          
        }
        
      } else {
        
        message(species, ' ', x, ' skipped - SDM not yet run') 
        
      }
      
    })
    
  })
  
}

```

\

The resulting maps generated by this function illustrate how climatic suitability varies across the landscape, 
with values of grid cells ranging from 0 (not suitable) to 1 (suitable, see Elith et al. 2011; Merow et al. 2013 
for more details of the  Maxent algorithm).

\newpage 
![](output/figures/CV_figs/A_implexa_CURRENT_SUIT.png)

**Figure 2.** Example of a continuous climatic suitabiliy map for one species under current condtions. 
Species occurrence points are plotted in red on the left panel. The cells in the centre and 
lower panels are coded from 0 - no to low suitability, to 1, highly suitable.

\


#########################################################################################################
## **Summarising the climatic suitability output within Significant Urban areas** 
For each species, we generated maps illustrating areas where suitable climate is projected to occur under 
current and future conditions. In these maps, a grid cell is given a value between 0 (highly unsuitable) 
and 1 (highly suitable). Using a species-specific threshold  -  the 10th percentile training presence 
logistic threshold  -  based on the weighting of different model errors ('commission' errors where 
a grid cell is classified as suitable when it is not, versus 'omission' errors where a grid cell 
is classified as unsuitable when it is suitable) these continuous suitability maps were converted 
into binary suitable/unsuitable maps (0 or 1). This resulted in six maps per species for each of 
the three future time periods - one map for each of the climate scenarios. The six maps were overlaid 
and summed such that the value of a grid cell could range from 0 (unsuitable in all climate scenarios) 
to 6 (suitable in all climate scenarios). Finally, maps were re-coded to quantify the number of grid 
cells that were classified as suitable in a least four of the six climate scenarios. These are grid 
cells that we have greater confidence will be climatically suitability for that species in the future.

\

We then calculated changes to the size of suitable climate in terms of i) overall change in size, 
ii) loss of currently suitable areas and iii) gain in new areas. Finally, we intersected species' maps 
with Significant Urban Areas (SUAs) from the 2016 Census. We then calculated the current area that 
is projected to be climatically suitable within all SUAs, and the extent to which this area may change 
under the future time periods (2030, 2050 and 2070). This was achieved by creating a third function, 
that counts cells lost or gained within an SUA (i.e. looping over each species and maxent threshold, 
for each time period, E.G. 2030)

\

```{r eval = FALSE, echo = TRUE}

#########################################################################################################
## A function which counts the number cells gained or lost for each species inside an SUA
suitability.2030 = tryCatch(mapply(SUA_cell_count,                      ## Function aggreagating GCMs by spatial unit
                                   unit_path     = "./DIR/",            ## Data path for the spatial unit 
                                   unit_file     = "SUA_2016_AUST.rds", ## Spatial unit of analysis - E.G. SUAs
                                   unit_vec      = "SUA_2016_VEC.rds",  ## Vector of rasterized unit cells
                                   DIR_list      = SDM.RESULTS.DIR,     ## List of directories with rasters
                                   species_list  = map_spp,             ## List of species' directories
                                   maxent_path   = maxent_path,         ## Directory of maxent results
                                   thresholds    = percent.10.log,      ## List of maxent thresholds
                                   percentiles   = percent.10.om,       ## 2nd List of maxent thresholds
                                   time_slice    = 30,                  ## Time period, eg 2030
                                   write_rasters = TRUE),               ## Save the combined rasters?
                            
                            error = function(cond) {
                              
                              message(paste('Species skipped - check inputs', spp))
                              
                            })

```

\

```{r eval = FALSE, echo = TRUE}

########################################################################
## Loop over directories, species and one threshold for each species
SUA_cell_count = function(unit_path, unit_file, unit_vec, 
                          DIR_list, species_list, 
                          maxent_path, thresholds, percentiles, 
                          time_slice, write_rasters) {

  ###################################################################################################################
  ## Read in shapefiles 
  areal_unit = readRDS(paste0(unit_path, unit_file)) %>%
    spTransform(ALB.CONICAL)
  areal_unit = areal_unit[order(areal_unit$SUA_NAME16),] 
  
  areal_unit_vec  = readRDS(paste0(unit_path, unit_vec)) 
  
  ## Loop over each directory
  lapply(DIR_list, function(DIR) { 
    
    ## And each species 
    lapply(species_list, function(species) {
      
      ####################################################################
      ## Create a list of the rasters in each directory, then take the mean. T
      message('Running summary of SDM predictions within SUAs for ', species, 
              ' using ', names(areal_unit)[1], " shapefile")
      message('Calcualting mean of 20', time_slice, ' GCMs for ', species)
      
      ## Check if the mean GCM raster exists
      f_mean = sprintf('%s/%s/full/%s_20%s_suitability_mean.tif', 
                       maxent_path, species, species, time_slice)
      
      ## The mean of the GCMs doesn't exist, create it
      if(!file.exists(f_mean)) { 
        
        raster.list       = list.files(as.character(DIR), 
                                       pattern = sprintf('bi%s.tif$', 
                                                         time_slice), full.names = TRUE)  
        suit              = stack(raster.list)
        suit.list         = unstack(suit)
        combo_suit_mean   = mean(suit)          

      } else {
        
        ## Create another level without the mean calculation
        raster.list = list.files(as.character(DIR), 
                                 pattern = sprintf('bi%s.tif$', time_slice), full.names = TRUE)  
        suit        = stack(raster.list)
        suit.list   = unstack(suit)
        
      }
      
      ######################################################################
      ## Then, create rasters that meet habitat suitability criteria thresholds
      for (thresh in thresholds) {
        
        ## Check if the SUA summary table exists
        SUA_file =   sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                             species, species, time_slice, "SUA_cell_count_", thresh)
        
        ## The mean of the GCMs doesn't exist, create it
        if(file.exists(SUA_file)) { 
          
          message(species, ' SUA_table already exists, skip')
          next
          
        }
        
        for (percent in percentiles) {
          
          ## Print the species being analysed
          message('doing ', species, ' | Logistic > ', thresh, ' for 20', time_slice)
          
          ## Read in the current suitability raster
          f_current <- raster(sprintf('%s/%s/full/%s_current.tif', 
                                      maxent_path, species, species))
          
          ## First, create a simple function to threshold each of the rasters in raster.list
          thresh_greater  = function (x) {x > thresh}
          percent_greater = function (x) {x > percent}
          
          ## Then apply this to just the current suitability raster. 
          ## Maximum training sensitivity plus specificity Logistic threshold
          ## 10th percentile training presence training omission
          current_suit_thresh  = thresh_greater(f_current)
          current_suit_percent = percent_greater(f_current) 
          
  
          ################################################################################
          ## First, calculate the cells which are greater that the: 
          ## Maximum training sensitivity plus specificity Logistic threshold
          message('Running thresholds for ', species, ' | 20', 
                  time_slice, ' combined suitability > ', thresh)
          
          suit_ras1_thresh   = thresh_greater(suit.list[[1]])   
          suit_ras2_thresh   = thresh_greater(suit.list[[2]])
          suit_ras3_thresh   = thresh_greater(suit.list[[3]])
          suit_ras4_thresh   = thresh_greater(suit.list[[4]])   
          suit_ras5_thresh   = thresh_greater(suit.list[[5]])
          suit_ras6_thresh   = thresh_greater(suit.list[[6]])
          
          ## Then calculate the cells which are greater than the 
          ## 10th percentile training presence training omission
          suit_ras1_percent  = percent_greater(suit.list[[1]])
          suit_ras2_percent  = percent_greater(suit.list[[2]])
          suit_ras3_percent  = percent_greater(suit.list[[3]])
          suit_ras4_percent  = percent_greater(suit.list[[4]])
          suit_ras5_percent  = percent_greater(suit.list[[5]])
          suit_ras6_percent  = percent_greater(suit.list[[6]])
          
          #################################################################################
          ## Then sum them up: All the threshholds
          combo_suit_thresh   =  Reduce("+", list(suit_ras1_thresh, suit_ras2_thresh, suit_ras3_thresh,
                                                  suit_ras4_thresh, suit_ras5_thresh, suit_ras6_thresh))
          
          ## All the percentiles
          combo_suit_percent  =  Reduce("+", list(suit_ras1_percent, suit_ras2_percent, suit_ras3_percent,
                                                  suit_ras4_percent, suit_ras5_percent, suit_ras6_percent))
          
          #################################################################################
          ## For each species, create a binary raster with cells > 4 GCMs above the maxent 
          ## threshold = 1, and cells with < 4 GCMs = 0. 
          message('Calculating change for ', species, ' | 20', time_slice, ' combined suitability > ', thresh)
          
          ## Functions for thresholding rasters
          band_4           <- function(x) {ifelse(x >=  4, 1, 0) }
          combo_suit_4GCM  <- calc(combo_suit_thresh, fun = band_4)
          
          ##################################################################################
          ## Now create a raster of the gain, loss and stable
          ## Create a raster stack of the current and future rasters
          message ("Counting cells lost/gained/stable/never suitable per SUA")
          
          ## Create a table of cell counts using a raster stack of current and future data
          d <- as.data.frame(stack(current_suit_thresh, combo_suit_4GCM)[]) %>% 
            setNames(c('current', 'future')) %>% 
            mutate(SUA_CODE16 = areal_unit_vec,
                   cell_number = seq_len(ncell(current_suit_thresh))) %>% 
            as.tbl
          dim(d);summary(d)
          
          ## Then classify the cells of the raster stack into lost, gained, stable and never
          d2 <- d %>% 
            na.omit %>% 
            
            mutate(lost   = current == 1 & future == 0,
                   gained = current == 0 & future == 1,
                   stable = current == 1 & future == 1,
                   never  = current == 0 & future == 0,
                   nodata = is.na(current) | is.na(future)) 
          d2$class <- apply(select(d2, lost:never), 1, which)
          dim(d2)
          
          ## Then group the cell counts by SUA
          d3 <- d2 %>% 
            group_by(SUA_CODE16) %>%
            
            summarize(CURRENT_SUITABLE = sum(current, na.rm = TRUE),
                      FUTURE_SUITABLE  = sum(future,  na.rm = TRUE),
                      LOST             = sum(lost,    na.rm = TRUE),
                      GAINED           = sum(gained,  na.rm = TRUE),
                      STABLE           = sum(stable,  na.rm = TRUE),
                      NEVER            = sum(never,   na.rm = TRUE),
                      NODAT            = sum(nodata,  na.rm = TRUE),
                      n_cells = n()) %>% 
            
            ## Then calculate change between current and future
            mutate(CHANGE    = FUTURE_SUITABLE - CURRENT_SUITABLE,
                   GAIN_LOSS = ifelse(CHANGE < 0, 'LOSS', ifelse(CHANGE > 0, 'GAIN', 'STABLE')),
                   GAIN_LOSS = ifelse(CURRENT_SUITABLE == 0 & FUTURE_SUITABLE == 0, 'NEVER', GAIN_LOSS))
          dim(d3)
          
          ## Add the species column
          d4 = d3 %>% 
            join(areal_unit@data, .) %>%
            add_column(., SPECIES = species,    .after = "AREASQKM16") %>%
            add_column(., PERIOD  = time_slice, .after = "SPECIES")    %>%
            add_column(., THRESH  = thresh,     .after = "PERIOD")
          View(d4)
          
          ##################################################################################
          ## Now calculate the number of cells lost/gained/stable across Australia
          message ("Counting cells lost/gained/stable/never suitable across Australia")
          d5 <- stack(current_suit_thresh, combo_suit_4GCM)[]
          r <- raster(current_suit_thresh)
          z <- as.data.frame(d4)
          
          ## Then classify the raster stack to make each value (i.e. outcome) unique
          r[d5[, 1]==1 & d5[, 2]==0] <- 1  ## 1 in current raster and 0 in future = LOSS
          r[d5[, 1]==0 & d5[, 2]==1] <- 2  ## 0 in current raster and 1 in future = GAIN
          r[d5[, 1]==1 & d5[, 2]==1] <- 3  ## 1 in current raster and 1 in future = STABLE
          r[d5[, 1]==0 & d5[, 2]==0] <- 4  ## 0 in current raster and 0 in future = NEVER_SUIT
          
          ## Now convert the raster to a factor and assign lables to the levels
          gain_loss <- as.factor(r)
          levels(gain_loss)[[1]] <- data.frame(ID = 1:4, 
                                               label = c('Lost', 'Gained', 'Stable', 'Never_Suitable'))
          z <- as.data.frame(d5)
          
          ## The gain/loss raster could be intersected with the SUAs
          gain_loss_table      = table(z[, 1], z[, 2])
          gain_loss_df         = as.data.frame(raster::freq(gain_loss))
          gain_loss_df$SPECIES = species
          gain_loss_df$PERIOD  = time_slice
          
          names(gain_loss_df)  = c("CHANGE", "COUNT", "SPECIES", "PERIOD")
          gain_loss_df         = gain_loss_df[, c("SPECIES", "PERIOD", "CHANGE", "COUNT")]
          
          ## Change values and remove the NA row
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 1] <- "LOST"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 2] <- "GAINED"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 3] <- "STABLE"
          gain_loss_df$CHANGE[gain_loss_df$CHANGE == 4] <- "NEVER_SUIT"
          gain_loss_df = head(gain_loss_df, 4)
          head(gain_loss_df)
          
          ##################################################################################
          ## Save the continental gain/loss table
          write.csv(gain_loss_df, sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                                          species, species, time_slice, "gain_loss_table_", thresh), 
                    row.names = FALSE)
          
          ##################################################################################
          ## Save the SUA gain/loss table
          write.csv(d4, sprintf('%s/%s/full/%s_20%s_%s%s.csv', maxent_path,
                                species, species, time_slice, "SUA_cell_count_", thresh), 
                    row.names = FALSE)
          
          ##################################################################################
          ## Now write the rasters
          ## If the rasters don't exist, write them for each species/threshold
          if(write_rasters == "TRUE") {
            
            ## Write the combined future raster with > 4 GCMs above the maximum training value
            message('Writing ', species, ' | 20', time_slice, ' 4 GCMs > ', percent)
            writeRaster(combo_suit_4GCM, sprintf('%s/%s/full/%s_20%s%s%s.tif', maxent_path,
                                                 species, species, time_slice, 
                                                 "_4GCMs_above_", thresh), overwrite = TRUE)
            
            ## Write out the gain/loss raster
            writeRaster(gain_loss, sprintf('%s/%s/full/%s_20%s%s%s.tif', maxent_path,
                                           species, species, time_slice, "_gain_loss_", thresh), 
                        datatype = 'INT2U', overwrite = TRUE)

          } else {
            
            message(' skip raster writing') 
            
          }
          
        }
        
      }
      
    })
    
  })
  
}

```


\newpage 
![](output/figures/CV_figs/A_implexa_FUTURE_SUIT.png)

**Figure 3.** Example of a combined suitabiliy map for one species under six climate models for 2030. 
Species occurrence points are plotted in red on the left panel. The cells in the left and lower panels 
are coded as cells where the species is predicted to be lost, gained, or remain stable.

\

#########################################################################################################
# **<span style="color:blue"> Results </span>**
#########################################################################################################

#########################################################################################################
### **Climatically suitable habitat for horticultural tree species** 

All 176 species received AUC and TSS values sufficiently high (i.e. AUC > 0.69 and TSS > 0.29) to indicate 
acceptable predictive power (see Table S1 in supplementary material). Averaged over all species, climate 
space across the 82 SUAs was predicted to span 16,150 km2 (± 9,082 km2) in the baseline period. However, 
we emphasize that the Maxent suitability surfaces have been thresholded: a higher or lower threshold will 
decrease or increase, respectively, the area classified as “suitable”. The spatial extent of climate space 
is predicted to decline to an average 13,043 (± 8,924 km2) by 2030, and 12,300 km2 (± 10,016 km2) by 2070. 
Of the 176 species, 73% are predicted to experience range declines: 18% are predicted to lose > 50% of 
climate space across their respective SUAs, by 2030, while 34% are predicted to lose > 50% by 2070. Among 
the 14 species (8%) predicted to have losses exceeding 90% of baseline climate space are Callitris oblonga 
(Pygmy cypress pine) and Acacia fimbriata (Brisbane golden wattle). In contrast, 11 species are predicted 
to experience increases in the extent of suitable climate space of more than 50%, e.g. Ficus platypoda 
(Rock fig) and Brachychiton rupestris (Queensland bottle tree).

\

Generally, suitable habitat is predicted to shift poleward. As such, species are likely to experience an 
extension of their southern (colder) range margin (e.g., Pongamia pinnata and Melia azedarach in Fig. 4), 
and/or a contraction of their northern (warmer) margin (e.g., Melia azedarach and Syzygium smithii in 
Fig. 4). However, some species  –  particularly those found along coastal strips of south-east Australia  
– may experience contraction at both range latitudinal margins (see Syzygium smithii, Fig. 4).


\newpage 
![](output/figures/CV_figs/FIG_4_PICS.png)

**Figure 4.** Examples of horticulturally-important tree species for which their 
climatic distributions are predicted to expand (Pongamia pinnata), shift (Melia azedarach) 
and shrink (Syzygium smithii) for 2030 (black areas) and 2070 (orange areas), relative 
to their baseline distributions (grey areas). Australia’s temperate SUAs are plotted 
in pink on the baseline map. Images are freely available in the public domain from 
Wikimedia Commons (CC BY-SA 3.0) and credited to Alpsdake (Melia azedarach), 
L. Shyamal [Pongamia pinnata (Millettia pinnata)] and Akos Kokai (Syzygium smithii).


\


#########################################################################################################
## **Climatically suitable habitat for tree species within urban areas** 


\

For the baseline period (1960-1990), SUAs were predicted to contain suitable climate for an average of 
74 of the 176 species (± 30), ranging from 10 species in Geraldton to 139 in Sydney. By 2070, this average 
is predicted to decline to 63 species (± 30), with ten SUAs (eight in eastern Queensland and two in 
south-west Western Australia) predicted to have climate space for 50% fewer species (Fig. 5). However, 
21 of the 82 SUAs may contain suitable climate for more species by 2070 than during the baseline period, 
including Orange (50% more species), Wangaratta (25%), and all five of the Tasmanian SUAs in this study 
(6-24%) - SUAs that predominantly lie in cooler regions. In addition, the spatial extent of suitable 
habitat across SUAs is generally predicted to decline. That is, averaged across their representative 
species, 12.3% (± 5.8%) of each SUA’s area will no longer be suitable by 2070, although average gains 
of 6.3% (± 4.3%) may occur elsewhere within the SUA. 

\

\newpage 
![](output/figures/CV_figs/FIG_2_barplots.png)

**Figure 5.** Based on predictions from a climate suitability model, barplots 
illustrate the percentage of native tree species with suitable habitat in the 
baseline period (1960-1990), that are predicted to experience gains (green) or 
losses (red) in the extent of suitable habitat over 82 Significant Urban Areas 
(SUA, left panels), and within the 19 largest SUAs (right panels), for 2030 
(a-b) and 2070 (c-d). SUAs are ranked by current mean annual temperature 
(MAT, e.g. Hobart is the coolest SUA, Mackay the the warmest)

\

SUAs with warmer baseline mean annual temperature 
are predicted to have fewer tree species experience increases in suitable habitat compared to cooler 
SUAs (Fig. 6). Similarly, the percent of tree species predicted to experience losses in suitable 
habitat increases with increasing mean annual temperature within SUAs. These patterns of change 
were consistent regardless of the area and population of the SUAs (Fig. 6). In addition, the deviance 
explained by GAMs in the long term (2070) was higher than the model deviance in the short term (2030).


\newpage 
![](output/figures/CV_figs/FIG_3_scatterplots.png)

**Figure 6.** The Scatterplots of the predicted percentage of original native 
tree species gains (top panels) and losses (bottom panels) within temperate SUAs, 
vs. the mean annual temperature of the SUA for both 2030 (lighter points) and 
2070 (darker points). The left panels are for all temperate SUAs, while the 
right panels are for the SUAs with area > 200 km2 and with population > 80,000. 
DE = deviance explained from generalised additive models (GAMs) of species 
gain/loss (y axis) in each SUA vs. the SUA MAT.

\



#########################################################################################################
# **<span style="color:blue"> Implications, caveats and future work </span>**
#########################################################################################################

\

Our analysis shows that commonly planted tree species are likely to respond to climate change 
within temperate urban areas in a similar fashion as they are predicted to respond within natural 
ecosystems – i.e. migrating pole-wards. Thus species found in SUAs in cooler areas are predicted 
to experience greater expansion of suitable climate compared to species in SUAs in warmer areas. 
(Figure 6). This increase of suitable habitat for more species in cooler urban areas is driven by 
the projection of a southwards (poleward) shift in climate space in the underlying climate models: 
species’ northern-most range margins are predicted to contract, while their southern-most margins 
may be extending into new areas.

\newpage 
![](output/figures/CV_figs/FIG_1_SUA_PLOT_crop.png)

**Figure 6.** The largest Significant Urban Areas (SUA) in each Australian 
State and Territory (Ade = Adelaide, Bris = Brisbane, Can = Canberra, Hob = Hobart, 
Mel = Melbourne; Per = Perth, Syd = Sydney - The Northern Territory was not included 
in this study, Australia’s temperate SUAs are plotted in pink). For each largest SUA, 
we plot the predicted change in average % of cells lost (orange), gained (green) and 
stable (light blue) in each SUA across all 176 species according to our climate 
suitability models, from baseline to 2070.

\


The repercussions of loss and gain for the Nursery Industry are two fold. Firstly, 
if an area becomes unsuitable for a species, this means that existing plants are 
likely to be vulnerable to climate change. Secondly, it signifies changing opportunities 
for growers: species that may have been solid performers in the past may be unreliable 
in the future, while new opportunities will emerge as suitable climate space appears 
beyond a species’ current range.  

\

While CSMs are useful tools for exploring the distribution of climatically suitable 
habitat, several factors should be kept in mind when interpreting their output 
- particularly when applied to plants growing in urban environments.

\

**1).** CSMs do not ‘predict’ where a species will occur. These tools identify where 
suitable conditions for the species occur with respect to the climatic variables used 
to calibrate the model. However, climatic extremes have been excluded from the model, 
which may alter the likelihood that a plant will survive in a given location.

\

**2).**  The models were generated at a spatial resolution of 1 x 1 km. At a finer scale, microclimatic 
characteristics also need to be considered when determining the suitability of a location for a species.

\

**3).**  CSMs provide an indication of the exposure of a species to climate change. However, they do not account 
for plasticity in the response of individuals to weather and climate. A species may be able to tolerate 
a broader range of conditions than is apparent from the distribution of its occurrence records. As such, 
the species may be able to survive in areas projected to be unsuitable.

\

In sum, CSMs are tools for developing a broad understanding of responses to climate change and are best 
used when combined with species-specific trait or experimental data.

\

#########################################################################################################
# **TBC...**
#########################################################################################################